{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Hyper-spectral Imaging Group at the University of Waikato.","title":"Home"},{"location":"people/","text":"Hyper-spectral imaging group: AProf Melanie Ooi Dale Fletcher Peter Reutemann","title":"People"},{"location":"publications/","text":"2023 # Abeysekera SK, Robinson A, Ooi MPL, Kuang YC, Manley-Harris M, Holmes W, Hirst E, Nowak J, Caddie M, Steinhorn G, Demidenko S. Sparse reproducible machine learning for near infrared hyperspectral imaging: Estimating the tetrahydrocannabinolic acid concentration in Cannabis sativa L. 1 Feb 2023. Industrial Crops and Products, 10.1016/j.indcrop.2022.116137","title":"Publications"},{"location":"publications/#2023","text":"Abeysekera SK, Robinson A, Ooi MPL, Kuang YC, Manley-Harris M, Holmes W, Hirst E, Nowak J, Caddie M, Steinhorn G, Demidenko S. Sparse reproducible machine learning for near infrared hyperspectral imaging: Estimating the tetrahydrocannabinolic acid concentration in Cannabis sativa L. 1 Feb 2023. Industrial Crops and Products, 10.1016/j.indcrop.2022.116137","title":"2023"},{"location":"happy/","text":"Applications, Python libraries and Docker images for hyper-spectral data processing and modelling. ADAMS happy-tools Segment Anything Segment Anything in High Quality Notes on WSL2 # The tools are all working under the Windows Subsystem for Linux. But Docker and graphical applications only work as long as you are using version 2 of WSL and your Windows 10 is at least build 19044 (no restrictions with Windows 11) . Here is how to switch WSL to version 2 as default by running the following command: wsl --set-default-version 2 You can view all your current images and what WSL version they are using with the following command: wsl --list -v If you want to switch an existing image (e.g., Ubuntu-22.04 ) to version 2, then you can run the following command: wsl --set-version Ubuntu-22.04 2 Source: https://stackoverflow.com/a/73164601","title":"Introduction"},{"location":"happy/#notes-on-wsl2","text":"The tools are all working under the Windows Subsystem for Linux. But Docker and graphical applications only work as long as you are using version 2 of WSL and your Windows 10 is at least build 19044 (no restrictions with Windows 11) . Here is how to switch WSL to version 2 as default by running the following command: wsl --set-default-version 2 You can view all your current images and what WSL version they are using with the following command: wsl --list -v If you want to switch an existing image (e.g., Ubuntu-22.04 ) to version 2, then you can run the following command: wsl --set-version Ubuntu-22.04 2 Source: https://stackoverflow.com/a/73164601","title":"Notes on WSL2"},{"location":"happy/adams/","text":"ADAMS-based framework that can be used for annotating images using its powerful workflow engine. Requirements # OpenJDK 11+ Windows: adoptium.net/temurin Debian/Ubuntu: sudo apt install openjdk-11-jdk Installation # Download a snapshot (in ZIP format) adams.cms.waikato.ac.nz/snapshots/happy/ Unzip the ZIP archive and rename the generated directory to happy-adams Starting the application # Start the user interface with: Windows: happy-adams\\bin\\start_gui.bat Linux: happy-adams/bin/start_gui.sh Available flows # Of the flows that are come with the Happy ADAMS framework, the following ones are relevant to the Happy project: adams-imaging-annotate_objects.flow - generating annotations for object detection (bounding box or polygon) adams-imaging-image_segmentation_annotation.flow - generating annotations for image segmentation (pixel-level classification) adams-imaging-ext_run-sam.flow - downloads and runs SAM (Segment Anything Model) via Docker (can be used within the above two flows as a separate annotation tool) Tutorials # Instructions on how to use these flows are available from the Applied Deep Learning website, specifically: object detection image segmentation Preview browser # The Preview browser (from the Visualization menu) can be used for viewing PNG/OPEX JSON files that were export from the envi-viewer tool. Using the ObjectLocationsFromReport with the OpexObjectLocationsReader you can generate an overlay of the annotations like this: The options used can be seen here: And here as a configuration setup that you can paste via the drop-down button in the top-right corner of the options dialog: # Project: adams # Date: 2023-08-23 16:26:05 # User: fracpete # Charset: UTF-8 # Modules: adams-core,adams-docker,adams-imaging,adams-imaging-ext,adams-json,adams-meta,adams-net,adams-redis,adams-spreadsheet,adams-xml # adams.gui.tools.previewbrowser.ObjectLocationsFromReport -image-reader adams.data.io.input.JAIImageReader -reader adams.data.io.input.OpexObjectLocationsReader -type-color-provider adams.gui.visualization.core.DefaultColorProvider -label-anchor MIDDLE_CENTER -shape-color-provider adams.gui.visualization.core.TranslucentColorProvider -provider adams.gui.visualization.core.DefaultColorProvider -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal adams.data.overlappingobjectremoval.PassThrough -show-object-panel true","title":"ADAMS"},{"location":"happy/adams/#requirements","text":"OpenJDK 11+ Windows: adoptium.net/temurin Debian/Ubuntu: sudo apt install openjdk-11-jdk","title":"Requirements"},{"location":"happy/adams/#installation","text":"Download a snapshot (in ZIP format) adams.cms.waikato.ac.nz/snapshots/happy/ Unzip the ZIP archive and rename the generated directory to happy-adams","title":"Installation"},{"location":"happy/adams/#starting-the-application","text":"Start the user interface with: Windows: happy-adams\\bin\\start_gui.bat Linux: happy-adams/bin/start_gui.sh","title":"Starting the application"},{"location":"happy/adams/#available-flows","text":"Of the flows that are come with the Happy ADAMS framework, the following ones are relevant to the Happy project: adams-imaging-annotate_objects.flow - generating annotations for object detection (bounding box or polygon) adams-imaging-image_segmentation_annotation.flow - generating annotations for image segmentation (pixel-level classification) adams-imaging-ext_run-sam.flow - downloads and runs SAM (Segment Anything Model) via Docker (can be used within the above two flows as a separate annotation tool)","title":"Available flows"},{"location":"happy/adams/#tutorials","text":"Instructions on how to use these flows are available from the Applied Deep Learning website, specifically: object detection image segmentation","title":"Tutorials"},{"location":"happy/adams/#preview-browser","text":"The Preview browser (from the Visualization menu) can be used for viewing PNG/OPEX JSON files that were export from the envi-viewer tool. Using the ObjectLocationsFromReport with the OpexObjectLocationsReader you can generate an overlay of the annotations like this: The options used can be seen here: And here as a configuration setup that you can paste via the drop-down button in the top-right corner of the options dialog: # Project: adams # Date: 2023-08-23 16:26:05 # User: fracpete # Charset: UTF-8 # Modules: adams-core,adams-docker,adams-imaging,adams-imaging-ext,adams-json,adams-meta,adams-net,adams-redis,adams-spreadsheet,adams-xml # adams.gui.tools.previewbrowser.ObjectLocationsFromReport -image-reader adams.data.io.input.JAIImageReader -reader adams.data.io.input.OpexObjectLocationsReader -type-color-provider adams.gui.visualization.core.DefaultColorProvider -label-anchor MIDDLE_CENTER -shape-color-provider adams.gui.visualization.core.TranslucentColorProvider -provider adams.gui.visualization.core.DefaultColorProvider -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal adams.data.overlappingobjectremoval.PassThrough -show-object-panel true","title":"Preview browser"},{"location":"happy/sam-hq/","text":"Segment Anything in High Quality (SAM-HQ) works like SAM and consists of pretrained models that perform image segmentation on RGB images and can aid the human in the annotation process. Prerequisites # Linux # docker redis-server ( sudo apt install redis-server ) Windows # WSL2 using Ubuntu 20.04 or 22.04 docker ( instructions ) redis-server ( sudo apt install redis-server ) Directories # sam-hq | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM-HQ models You can create the structure using the following command: mkdir -p sam-hq/cache \\ mkdir -p sam-hq/models Pretrained models # Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam-hq/models directory, run the following command: wget https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_l.pth Service scripts (WSL2 without Docker Desktop UI) # Create a bash script happy_samhq_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_samhq_start.sh Create a bash script happy_samhq_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happyhq_sam_stop.sh SAM scripts # In the sam-hq directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam-hq:2023-08-17_cuda11.6 \\ samhq_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_hq_vit_l.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . NB: This script uses the sam_in and sam_out Redis channels to make it a drop-in replacement for SAM in the envi-viewer . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]amhq_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh . Starting # Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_samhq_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background. SAM # In the sam-hq directory, execute the start.sh script. Stopping # SAM # In the sam-hq directory, execute the stop.sh script. Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_samhq_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"SAM-HQ"},{"location":"happy/sam-hq/#prerequisites","text":"","title":"Prerequisites"},{"location":"happy/sam-hq/#linux","text":"docker redis-server ( sudo apt install redis-server )","title":"Linux"},{"location":"happy/sam-hq/#windows","text":"WSL2 using Ubuntu 20.04 or 22.04 docker ( instructions ) redis-server ( sudo apt install redis-server )","title":"Windows"},{"location":"happy/sam-hq/#directories","text":"sam-hq | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM-HQ models You can create the structure using the following command: mkdir -p sam-hq/cache \\ mkdir -p sam-hq/models","title":"Directories"},{"location":"happy/sam-hq/#pretrained-models","text":"Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam-hq/models directory, run the following command: wget https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_l.pth","title":"Pretrained models"},{"location":"happy/sam-hq/#service-scripts-wsl2-without-docker-desktop-ui","text":"Create a bash script happy_samhq_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_samhq_start.sh Create a bash script happy_samhq_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happyhq_sam_stop.sh","title":"Service scripts (WSL2 without Docker Desktop UI)"},{"location":"happy/sam-hq/#sam-scripts","text":"In the sam-hq directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam-hq:2023-08-17_cuda11.6 \\ samhq_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_hq_vit_l.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . NB: This script uses the sam_in and sam_out Redis channels to make it a drop-in replacement for SAM in the envi-viewer . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]amhq_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh .","title":"SAM scripts"},{"location":"happy/sam-hq/#starting","text":"","title":"Starting"},{"location":"happy/sam-hq/#docker-and-redis-wsl2-without-docker-desktop-ui","text":"sudo /usr/local/bin/happy_samhq_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/sam-hq/#sam","text":"In the sam-hq directory, execute the start.sh script.","title":"SAM"},{"location":"happy/sam-hq/#stopping","text":"","title":"Stopping"},{"location":"happy/sam-hq/#sam_1","text":"In the sam-hq directory, execute the stop.sh script.","title":"SAM"},{"location":"happy/sam-hq/#docker-and-redis-wsl2-without-docker-desktop-ui_1","text":"sudo /usr/local/bin/happy_samhq_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/","text":"Facebook's Segment Anything (SAM) are pretrained models that perform image segmentation on RGB images and can aid the human in the annotation process. Prerequisites # Linux # docker redis-server ( sudo apt install redis-server ) Windows # WSL2 using Ubuntu 20.04 or 22.04 docker ( instructions ) redis-server ( sudo apt install redis-server ) Directories # sam | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM models You can create the structure using the following command: mkdir -p sam/cache \\ mkdir -p sam/models Pretrained models # Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam/models directory, run the following command: wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth Service scripts (WSL2 without Docker Desktop UI) # Create a bash script happy_sam_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_sam_start.sh Create a bash script happy_sam_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happy_sam_stop.sh SAM scripts # In the sam directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam:2023-04-16_cuda11.6 \\ sam_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_vit_l_0b3195.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]am_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh . Starting # Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_sam_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background. SAM # In the sam directory, execute the start.sh script. Stopping # SAM # In the sam directory, execute the stop.sh script. Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_sam_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"SAM"},{"location":"happy/sam/#prerequisites","text":"","title":"Prerequisites"},{"location":"happy/sam/#linux","text":"docker redis-server ( sudo apt install redis-server )","title":"Linux"},{"location":"happy/sam/#windows","text":"WSL2 using Ubuntu 20.04 or 22.04 docker ( instructions ) redis-server ( sudo apt install redis-server )","title":"Windows"},{"location":"happy/sam/#directories","text":"sam | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM models You can create the structure using the following command: mkdir -p sam/cache \\ mkdir -p sam/models","title":"Directories"},{"location":"happy/sam/#pretrained-models","text":"Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam/models directory, run the following command: wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth","title":"Pretrained models"},{"location":"happy/sam/#service-scripts-wsl2-without-docker-desktop-ui","text":"Create a bash script happy_sam_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_sam_start.sh Create a bash script happy_sam_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happy_sam_stop.sh","title":"Service scripts (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/#sam-scripts","text":"In the sam directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam:2023-04-16_cuda11.6 \\ sam_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_vit_l_0b3195.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]am_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh .","title":"SAM scripts"},{"location":"happy/sam/#starting","text":"","title":"Starting"},{"location":"happy/sam/#docker-and-redis-wsl2-without-docker-desktop-ui","text":"sudo /usr/local/bin/happy_sam_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/#sam","text":"In the sam directory, execute the start.sh script.","title":"SAM"},{"location":"happy/sam/#stopping","text":"","title":"Stopping"},{"location":"happy/sam/#sam_1","text":"In the sam directory, execute the stop.sh script.","title":"SAM"},{"location":"happy/sam/#docker-and-redis-wsl2-without-docker-desktop-ui_1","text":"sudo /usr/local/bin/happy_sam_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/happy_tools/","text":"happy-tools contains several command-line utilities and graphical viewers for HSI files: envi-viewer - for viewing ENVI HSI images happy-hsi2csv - converts HSI images into CSV happy-hsi2rbg - generates fake RGB PNG files from HSI images happy-opex2happy - converts OPEX JSON annotations and PNG images into happy data structures These tools are available from the Python virtual environment that they were installed. E.g., when following the installation instructions on this website, the tools would be located in the following directory in the user's home folder: happy/bin","title":"Overview"},{"location":"happy/happy_tools/envi-viewer/","text":"Usage # Image tab # Via the File menu, you can load an ENVI file representing a sample scan. From that menu, you can also select black and white reference ENVI files that get automatically applied to the scan: At the bottom of the window, you get a quick info on what dimensions the scan has (width, height and channels). The three sliders allow you to select the channels from the hyper-spectral image to act as red, green and blue channel for the fake RGB image that is being displayed. Left-clicking on the label next to the slider, depicting the current channel value, pops up a dialog for entering a specific channel. If default bands are defined in the ENVI header and auto-detect channels is enabled on the Options tab , then these will get used when loading the file. Info tab # On the Info tab, you can see what files are currently loaded and what dimensions these files have: Options tab # On the Options tab, you can change various view settings, how to connect to SAM and how annotations appear: Annotations # The envi-viewer also allows you to annotate images and then export them. The image will be exported as PNG using the currently selected channels. Any annotations present will get exported as JSON, using the OPEX format . Such annotations in OPEX format can be viewed in the ADAMS Preview browser . General mouse usage: Left-clicking on the image sets a marker point. Left-clicking while holding the CTRL key removes any marker points. Left-clicking on an existing annotation shape while holding the SHIFT key allows you to enter a label for that shape (e.g., white_ref or leaf ). Tools menu: Clear annotations - removes any annotations Clear markers - removes all marker points Remove last annotation - removes the most recent annotation that was added (can be repeated till there are no more annotations) Polygon - turns current marker points into polygon SAM - uses current marker points as prompt points for SAM Polygons # The simplest way of annotating that does not require any further tools is by using polygons. First define the outline of the object with marker points: Once at least three marker points have been put on the image, selecting Polygon from the Tools menu turns them into a polygon annotation: SAM # Using SAM , you can easily annotate complex shapes accurately. Though SAM can run on a CPU, it is recommended to use a computer with a NVIDIA GPU as it will speed up the detection process by at least 10 times. SAM requires you to at least provide a single marker on the object that you want to trace the shape for. Depending on the object and how well it is separated from the background, how much the colors on the object change, you may have to provide more than one marker point to better guide the detection: The result looks then like this: Command-line # Using the command-line options, you can preset the options in the user interface and also load scan, black and white reference files: usage: envi-viewer [-h] [-s SCAN] [-f BLACK_REFERENCE] [-w WHITE_REFERENCE] [-r INT] [-g INT] [-b INT] [--autodetect_channels] [--keep_aspectratio] [--annotation_color HEXCOLOR] [--redis_host HOST] [--redis_port PORT] [--redis_pw PASSWORD] [--redis_in CHANNEL] [--redis_out CHANNEL] [--redis_connect] [--marker_size INT] [--marker_color HEXCOLOR] [--min_obj_size INT] ENVI Hyper-spectral Image Viewer. Offers contour detection using SAM (Segment- Anything: https://github.com/waikato-datamining/pytorch/tree/master/segment- anything) optional arguments: -h, --help show this help message and exit -s SCAN, --scan SCAN Path to the scan file (ENVI format) (default: None) -f BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -r INT, --scale_r INT the wave length to use for the red channel (default: None) -g INT, --scale_g INT the wave length to use for the green channel (default: None) -b INT, --scale_b INT the wave length to use for the blue channel (default: None) --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: None) --keep_aspectratio whether to keep the aspect ratio (default: None) --annotation_color HEXCOLOR the color to use for the annotations like contours (hex color) (default: None) --redis_host HOST The Redis host to connect to (IP or hostname) (default: None) --redis_port PORT The port the Redis server is listening on (default: None) --redis_pw PASSWORD The password to use with the Redis server (default: None) --redis_in CHANNEL The channel that SAM is receiving images on (default: None) --redis_out CHANNEL The channel that SAM is broadcasting the detections on (default: None) --redis_connect whether to immediately connect to the Redis host (default: None) --marker_size INT The size in pixels for the SAM points (default: None) --marker_color HEXCOLOR the color to use for the SAM points (hex color) (default: None) --min_obj_size INT The minimum size that SAM contours need to have (<= 0 for no minimum) (default: None)","title":"envi-viewer"},{"location":"happy/happy_tools/envi-viewer/#usage","text":"","title":"Usage"},{"location":"happy/happy_tools/envi-viewer/#image-tab","text":"Via the File menu, you can load an ENVI file representing a sample scan. From that menu, you can also select black and white reference ENVI files that get automatically applied to the scan: At the bottom of the window, you get a quick info on what dimensions the scan has (width, height and channels). The three sliders allow you to select the channels from the hyper-spectral image to act as red, green and blue channel for the fake RGB image that is being displayed. Left-clicking on the label next to the slider, depicting the current channel value, pops up a dialog for entering a specific channel. If default bands are defined in the ENVI header and auto-detect channels is enabled on the Options tab , then these will get used when loading the file.","title":"Image tab"},{"location":"happy/happy_tools/envi-viewer/#info-tab","text":"On the Info tab, you can see what files are currently loaded and what dimensions these files have:","title":"Info tab"},{"location":"happy/happy_tools/envi-viewer/#options-tab","text":"On the Options tab, you can change various view settings, how to connect to SAM and how annotations appear:","title":"Options tab"},{"location":"happy/happy_tools/envi-viewer/#annotations","text":"The envi-viewer also allows you to annotate images and then export them. The image will be exported as PNG using the currently selected channels. Any annotations present will get exported as JSON, using the OPEX format . Such annotations in OPEX format can be viewed in the ADAMS Preview browser . General mouse usage: Left-clicking on the image sets a marker point. Left-clicking while holding the CTRL key removes any marker points. Left-clicking on an existing annotation shape while holding the SHIFT key allows you to enter a label for that shape (e.g., white_ref or leaf ). Tools menu: Clear annotations - removes any annotations Clear markers - removes all marker points Remove last annotation - removes the most recent annotation that was added (can be repeated till there are no more annotations) Polygon - turns current marker points into polygon SAM - uses current marker points as prompt points for SAM","title":"Annotations"},{"location":"happy/happy_tools/envi-viewer/#polygons","text":"The simplest way of annotating that does not require any further tools is by using polygons. First define the outline of the object with marker points: Once at least three marker points have been put on the image, selecting Polygon from the Tools menu turns them into a polygon annotation:","title":"Polygons"},{"location":"happy/happy_tools/envi-viewer/#sam","text":"Using SAM , you can easily annotate complex shapes accurately. Though SAM can run on a CPU, it is recommended to use a computer with a NVIDIA GPU as it will speed up the detection process by at least 10 times. SAM requires you to at least provide a single marker on the object that you want to trace the shape for. Depending on the object and how well it is separated from the background, how much the colors on the object change, you may have to provide more than one marker point to better guide the detection: The result looks then like this:","title":"SAM"},{"location":"happy/happy_tools/envi-viewer/#command-line","text":"Using the command-line options, you can preset the options in the user interface and also load scan, black and white reference files: usage: envi-viewer [-h] [-s SCAN] [-f BLACK_REFERENCE] [-w WHITE_REFERENCE] [-r INT] [-g INT] [-b INT] [--autodetect_channels] [--keep_aspectratio] [--annotation_color HEXCOLOR] [--redis_host HOST] [--redis_port PORT] [--redis_pw PASSWORD] [--redis_in CHANNEL] [--redis_out CHANNEL] [--redis_connect] [--marker_size INT] [--marker_color HEXCOLOR] [--min_obj_size INT] ENVI Hyper-spectral Image Viewer. Offers contour detection using SAM (Segment- Anything: https://github.com/waikato-datamining/pytorch/tree/master/segment- anything) optional arguments: -h, --help show this help message and exit -s SCAN, --scan SCAN Path to the scan file (ENVI format) (default: None) -f BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -r INT, --scale_r INT the wave length to use for the red channel (default: None) -g INT, --scale_g INT the wave length to use for the green channel (default: None) -b INT, --scale_b INT the wave length to use for the blue channel (default: None) --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: None) --keep_aspectratio whether to keep the aspect ratio (default: None) --annotation_color HEXCOLOR the color to use for the annotations like contours (hex color) (default: None) --redis_host HOST The Redis host to connect to (IP or hostname) (default: None) --redis_port PORT The port the Redis server is listening on (default: None) --redis_pw PASSWORD The password to use with the Redis server (default: None) --redis_in CHANNEL The channel that SAM is receiving images on (default: None) --redis_out CHANNEL The channel that SAM is broadcasting the detections on (default: None) --redis_connect whether to immediately connect to the Redis host (default: None) --marker_size INT The size in pixels for the SAM points (default: None) --marker_color HEXCOLOR the color to use for the SAM points (hex color) (default: None) --min_obj_size INT The minimum size that SAM contours need to have (<= 0 for no minimum) (default: None)","title":"Command-line"},{"location":"happy/happy_tools/happy-hsi2csv/","text":"Command-line # usage: happy-hsi2csv [-h] -d DIR -m DIR -s FILE -o DIR [-M [METADATA_VALUES [METADATA_VALUES ...]]] -T TARGETS [TARGETS ...] Generates CSV files from hyper-spectral images. optional arguments: -h, --help show this help message and exit -d DIR, --data_dir DIR the directory with the hyper-spectral data files (default: None) -m DIR, --metadata_dir DIR the directory with the meta-data JSON files (default: None) -s FILE, --sample_ids FILE the JSON file with the array of sample IDs to process (default: None) -o DIR, --output_dir DIR the directory to store the results in (default: None) -M [METADATA_VALUES [METADATA_VALUES ...]], --metadata_values [METADATA_VALUES [METADATA_VALUES ...]] the meta-data values to add to the output (default: None) -T TARGETS [TARGETS ...], --targets TARGETS [TARGETS ...] the target values to generate data for (default: None)","title":"happy-hsi2csv"},{"location":"happy/happy_tools/happy-hsi2csv/#command-line","text":"usage: happy-hsi2csv [-h] -d DIR -m DIR -s FILE -o DIR [-M [METADATA_VALUES [METADATA_VALUES ...]]] -T TARGETS [TARGETS ...] Generates CSV files from hyper-spectral images. optional arguments: -h, --help show this help message and exit -d DIR, --data_dir DIR the directory with the hyper-spectral data files (default: None) -m DIR, --metadata_dir DIR the directory with the meta-data JSON files (default: None) -s FILE, --sample_ids FILE the JSON file with the array of sample IDs to process (default: None) -o DIR, --output_dir DIR the directory to store the results in (default: None) -M [METADATA_VALUES [METADATA_VALUES ...]], --metadata_values [METADATA_VALUES [METADATA_VALUES ...]] the meta-data values to add to the output (default: None) -T TARGETS [TARGETS ...], --targets TARGETS [TARGETS ...] the target values to generate data for (default: None)","title":"Command-line"},{"location":"happy/happy_tools/happy-hsi2rbg/","text":"Command-line # usage: happy-hsi2rgb [-h] -i INPUT_DIR [INPUT_DIR ...] [-r] [-e EXTENSION] [-b BLACK_REFERENCE] [-w WHITE_REFERENCE] [-a] [--red INT] [--green INT] [--blue INT] [-o OUTPUT_DIR] [--width INT] [--height INT] [-n] [-v] Fake RGB image generator for HSI files. optional arguments: -h, --help show this help message and exit -i INPUT_DIR [INPUT_DIR ...], --input_dir INPUT_DIR [INPUT_DIR ...] Path to the scan file (ENVI format) (default: None) -r, --recursive whether to traverse the directories recursively (default: False) -e EXTENSION, --extension EXTENSION The file extension to look for (default: .hdr) -b BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -a, --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: False) --red INT the wave length to use for the red channel (0-based) (default: 0) --green INT the wave length to use for the green channel (0-based) (default: 0) --blue INT the wave length to use for the blue channel (0-based) (default: 0) -o OUTPUT_DIR, --output_dir OUTPUT_DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) --width INT the width to scale the images to (<= 0 uses image dimension) (default: 0) --height INT the height to scale the images to (<= 0 uses image dimension) (default: 0) -n, --dry_run whether to omit saving the PNG images (default: False) -v, --verbose whether to be more verbose with the output (default: False)","title":"happy-hsi2rbg"},{"location":"happy/happy_tools/happy-hsi2rbg/#command-line","text":"usage: happy-hsi2rgb [-h] -i INPUT_DIR [INPUT_DIR ...] [-r] [-e EXTENSION] [-b BLACK_REFERENCE] [-w WHITE_REFERENCE] [-a] [--red INT] [--green INT] [--blue INT] [-o OUTPUT_DIR] [--width INT] [--height INT] [-n] [-v] Fake RGB image generator for HSI files. optional arguments: -h, --help show this help message and exit -i INPUT_DIR [INPUT_DIR ...], --input_dir INPUT_DIR [INPUT_DIR ...] Path to the scan file (ENVI format) (default: None) -r, --recursive whether to traverse the directories recursively (default: False) -e EXTENSION, --extension EXTENSION The file extension to look for (default: .hdr) -b BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -a, --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: False) --red INT the wave length to use for the red channel (0-based) (default: 0) --green INT the wave length to use for the green channel (0-based) (default: 0) --blue INT the wave length to use for the blue channel (0-based) (default: 0) -o OUTPUT_DIR, --output_dir OUTPUT_DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) --width INT the width to scale the images to (<= 0 uses image dimension) (default: 0) --height INT the height to scale the images to (<= 0 uses image dimension) (default: 0) -n, --dry_run whether to omit saving the PNG images (default: False) -v, --verbose whether to be more verbose with the output (default: False)","title":"Command-line"},{"location":"happy/happy_tools/happy-opex2happy/","text":"Command-line # usage: happy-opex2happy [-h] -i DIR [DIR ...] [-o DIR] -f {flat,dir-tree} -l LABELS [-I] [-n] [-v] Turns annotations (PNG and OPEX JSON) into Happy ENVI format. optional arguments: -h, --help show this help message and exit -i DIR [DIR ...], --input_dir DIR [DIR ...] Path to the PNG/OPEX files (default: None) -o DIR, --output_dir DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) -f {flat,dir-tree}, --output_format {flat,dir-tree} Defines how to store the data in the output directory. (default: flat) -l LABELS, --labels LABELS The comma-separated list of object labels to export ('Background' is automatically added). (default: None) -I, --include_input whether to copy the PNG/JSON file across to the output dir (default: False) -n, --dry_run whether to omit saving the PNG images (default: False) -v, --verbose whether to be more verbose with the output (default: False)","title":"happy-opex2happy"},{"location":"happy/happy_tools/happy-opex2happy/#command-line","text":"usage: happy-opex2happy [-h] -i DIR [DIR ...] [-o DIR] -f {flat,dir-tree} -l LABELS [-I] [-n] [-v] Turns annotations (PNG and OPEX JSON) into Happy ENVI format. optional arguments: -h, --help show this help message and exit -i DIR [DIR ...], --input_dir DIR [DIR ...] Path to the PNG/OPEX files (default: None) -o DIR, --output_dir DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) -f {flat,dir-tree}, --output_format {flat,dir-tree} Defines how to store the data in the output directory. (default: flat) -l LABELS, --labels LABELS The comma-separated list of object labels to export ('Background' is automatically added). (default: None) -I, --include_input whether to copy the PNG/JSON file across to the output dir (default: False) -n, --dry_run whether to omit saving the PNG images (default: False) -v, --verbose whether to be more verbose with the output (default: False)","title":"Command-line"},{"location":"happy/happy_tools/installation/","text":"Prerequisites # Windows: install Ubuntu 2022.04.x from the Microsoft store (requires up-to-date WSL2 on Windows 10 Build 19044+ or Windows 11 ). Python Virtual environments sudo apt install virtualenv python3-tk Installation # In the home directory, create a Python virtual environment in directory happy with access to the system-wide installed libraries: virtualenv --system-site-packages -p /usr/bin/python3 happy Install the happy-tools straight from the repository: ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git Updating # Once installed, you can update the library as follows: ./happy/bin/pip uninstall -y happy-tools ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git Uninstall # You can completely remove the tools by simply deleting the happy directory: rm -Rf ./happy","title":"Installation"},{"location":"happy/happy_tools/installation/#prerequisites","text":"Windows: install Ubuntu 2022.04.x from the Microsoft store (requires up-to-date WSL2 on Windows 10 Build 19044+ or Windows 11 ). Python Virtual environments sudo apt install virtualenv python3-tk","title":"Prerequisites"},{"location":"happy/happy_tools/installation/#installation","text":"In the home directory, create a Python virtual environment in directory happy with access to the system-wide installed libraries: virtualenv --system-site-packages -p /usr/bin/python3 happy Install the happy-tools straight from the repository: ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git","title":"Installation"},{"location":"happy/happy_tools/installation/#updating","text":"Once installed, you can update the library as follows: ./happy/bin/pip uninstall -y happy-tools ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git","title":"Updating"},{"location":"happy/happy_tools/installation/#uninstall","text":"You can completely remove the tools by simply deleting the happy directory: rm -Rf ./happy","title":"Uninstall"}]}