{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the Hyperspectral Imaging Group at the University of Waikato. We are a team of engineers and computer scientists dedicated to advancing the hyperspectral imaging (HSI) technology through open-source platforms, software-hardware integration, machine learning and AI-informed image acquisition protocols. Our group serves as a hub for collaboration, exploration, and discovery in the hyperpectral imaging field. HAPPy Software # At the core of our research is the HAPPy software, which is abbreviation for Hyperspectral Applications Platform in Python. This is a groundbreaking platform that streamlines hyperspectral data acquisition, annotation, analysis, and the exploration of machine learning algorithms. HAPPy empowers researchers to bridge the gap between hyperspectral data and state-of-the-art machine learning algorithms, facilitating quicker experimentation and innovation in HSI. Our Hardware # We employ state-of-the-art hardware, including hyperspectral cameras and rotary stages, to capture high-quality hyperspectral data. Our hardware setup is carefully designed to ensure precision and accuracy in data acquisition, providing a solid foundation for our research endeavors. Publications # Our commitment to excellence in hyperspectral imaging research is reflected in our publications. Explore our publications to gain insights into our latest findings and contributions to the field.","title":"Home"},{"location":"#happy-software","text":"At the core of our research is the HAPPy software, which is abbreviation for Hyperspectral Applications Platform in Python. This is a groundbreaking platform that streamlines hyperspectral data acquisition, annotation, analysis, and the exploration of machine learning algorithms. HAPPy empowers researchers to bridge the gap between hyperspectral data and state-of-the-art machine learning algorithms, facilitating quicker experimentation and innovation in HSI.","title":"HAPPy Software"},{"location":"#our-hardware","text":"We employ state-of-the-art hardware, including hyperspectral cameras and rotary stages, to capture high-quality hyperspectral data. Our hardware setup is carefully designed to ensure precision and accuracy in data acquisition, providing a solid foundation for our research endeavors.","title":"Our Hardware"},{"location":"#publications","text":"Our commitment to excellence in hyperspectral imaging research is reflected in our publications. Explore our publications to gain insights into our latest findings and contributions to the field.","title":"Publications"},{"location":"hardware/","text":"The hyperspectral imaging hardware available: Specim FX17 camera operates in the near-infrared region, with free wavelength selection from 224 bands within the camera coverage. Specim RS10 rotary stage scanner allows scanning an image of a stationary target or scenery in the lab and field. Manuals # Specim FX17 User Manual Specim RS10 User Guide LUMO Scanner Setup #","title":"Hardware"},{"location":"hardware/#manuals","text":"Specim FX17 User Manual Specim RS10 User Guide LUMO Scanner","title":"Manuals"},{"location":"hardware/#setup","text":"","title":"Setup"},{"location":"publications/","text":"2023 # Holmes, W.S., Ooi, M.P.L., Abeysekera, S., Kuang, Y.C., Simpkin, R., Caddie, M., Nowak, J. and Demidenko, S., 2023, May. On machine learning methods to estimate cannabidiolic acid content of Cannabis sativa L. from near-infrared hyperspectral imaging. In 2023 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) (pp. 01-06). IEEE. 10.1109/I2MTC53148.2023.10175994 Abeysekera, S.K., Robinson, A., Ooi, M.P.L., Kuang, Y.C., Manley-Harris, M., Holmes, W., Hirst, E., Nowak, J., Caddie, M., Steinhorn, G. and Demidenko, S., 2023. Sparse reproducible machine learning for near infrared hyperspectral imaging: Estimating the tetrahydrocannabinolic acid concentration in Cannabis sativa L. Industrial Crops and Products, 192, p.116137. 10.1016/j.indcrop.2022.116137 2022 # Ooi, M.P.L., Robinson, A., Manley-Harris, M., Hill, S., Raymond, L., Kuang, Y.C., Steinhorn, G., Caddie, M., Nowak, J., Holmes, W. and Demidenko, S., 2022. Robust statistical analysis to predict and estimate the concentration of the cannabidiolic acid in Cannabis sativa L.: A comparative study. Industrial Crops and Products, 189, p.115744. 10.1016/j.indcrop.2022.115744 2020 # Holmes, W.S., Ooi, M.P.L., Kuang, Y.C., Simpkin, R., Blanchon, D., Gupta, G.S. and Demidenko, S., 2020, May. Signal-to-noise ratio contributors and effects in proximal near-infrared spectral reflectance measurement on plant leaves. In 2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) (pp. 1-6). IEEE. 10.1109/I2MTC43012.2020.9129359 Holmes, W.S., Ooi, M.P.L., Kuang, Y.C., Simpkin, R., Lopez-Ubiria, I., Vidiella, A., Blanchon, D., Gupta, G.S. and Demidenko, S., 2020, May. Classifying Cannabis sativa flowers, stems and leaves using statistical machine learning with near-infrared hyperspectral reflectance imaging. In 2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) (pp. 1-6). IEEE. 10.1109/I2MTC43012.2020.9129531","title":"Publications"},{"location":"publications/#2023","text":"Holmes, W.S., Ooi, M.P.L., Abeysekera, S., Kuang, Y.C., Simpkin, R., Caddie, M., Nowak, J. and Demidenko, S., 2023, May. On machine learning methods to estimate cannabidiolic acid content of Cannabis sativa L. from near-infrared hyperspectral imaging. In 2023 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) (pp. 01-06). IEEE. 10.1109/I2MTC53148.2023.10175994 Abeysekera, S.K., Robinson, A., Ooi, M.P.L., Kuang, Y.C., Manley-Harris, M., Holmes, W., Hirst, E., Nowak, J., Caddie, M., Steinhorn, G. and Demidenko, S., 2023. Sparse reproducible machine learning for near infrared hyperspectral imaging: Estimating the tetrahydrocannabinolic acid concentration in Cannabis sativa L. Industrial Crops and Products, 192, p.116137. 10.1016/j.indcrop.2022.116137","title":"2023"},{"location":"publications/#2022","text":"Ooi, M.P.L., Robinson, A., Manley-Harris, M., Hill, S., Raymond, L., Kuang, Y.C., Steinhorn, G., Caddie, M., Nowak, J., Holmes, W. and Demidenko, S., 2022. Robust statistical analysis to predict and estimate the concentration of the cannabidiolic acid in Cannabis sativa L.: A comparative study. Industrial Crops and Products, 189, p.115744. 10.1016/j.indcrop.2022.115744","title":"2022"},{"location":"publications/#2020","text":"Holmes, W.S., Ooi, M.P.L., Kuang, Y.C., Simpkin, R., Blanchon, D., Gupta, G.S. and Demidenko, S., 2020, May. Signal-to-noise ratio contributors and effects in proximal near-infrared spectral reflectance measurement on plant leaves. In 2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) (pp. 1-6). IEEE. 10.1109/I2MTC43012.2020.9129359 Holmes, W.S., Ooi, M.P.L., Kuang, Y.C., Simpkin, R., Lopez-Ubiria, I., Vidiella, A., Blanchon, D., Gupta, G.S. and Demidenko, S., 2020, May. Classifying Cannabis sativa flowers, stems and leaves using statistical machine learning with near-infrared hyperspectral reflectance imaging. In 2020 IEEE International Instrumentation and Measurement Technology Conference (I2MTC) (pp. 1-6). IEEE. 10.1109/I2MTC43012.2020.9129531","title":"2020"},{"location":"team/","text":"The Hyperspectral imaging team: Associate Professor Melanie Ooi Senior Lecturer Ye Chow Kuang Professor Geoffrey Holmes Dr Sanush Abeysekera Mr Dale Fletcher Mr Peter Reutemann","title":"Team"},{"location":"happy/","text":"Overview # HAPPy, short for \"Hyperspectral Application Platform in Python,\" stands as a pioneering platform at the forefront of hyperspectral imaging technology. Designed by the Hyperspectral Imaging Group at the University of Waikato, HAPPy unifies hyperspectral workflows for how hyperspectral images are acquired, processed, and analyzed. At its core, HAPPy seamlessly integrates data acquisition, annotation, analysis, and the exploration of novel statistical machine learning and deep learning approaches. Key Features # Containerized Algorithm Management: HAPPy simplifies algorithm deployment, ensuring compatibility with open-source machine learning platforms. Hardware-Software Calibration Protocols: Real-time machine learning model assessment at the imaging front enhances adaptability. Efficiency in Hyperspectral Workflow: Accelerated experimentation allows quick feasibility assessment and method shortlisting for development. Empowering Innovation: HAPPy empowers researchers to bridge the gap between hyperspectral data and state-of-the-art machine learning algorithms. It is the tool that fosters quicker experimentation, innovation, and discovery within the dynamic field of hyperspectral imaging. Open Source and Collaborative: Committed to the principles of open-source collaboration, we invite researchers, engineers, and enthusiasts from around the world to explore, contribute, and leverage the HAPPy. Get Started: Whether you're a seasoned hyperspectral imaging expert or just embarking on your journey, HAPPy is here to simplify, accelerate, and amplify your research efforts. Scroll down for information on installation and usage, as well as access to our tools and publications. Installation # The tools have been tested on Linux (Ubuntu/Debian) and under Windows with WSL2 using Ubuntu 22.04.x. Notes on WSL2 Installing the tools Usage # Once installed using the above instructions, you can launch graphical tools and Docker images (start/stop) using the happy-launch.sh script from your home directory: ./happy-launch.sh Tools # More details on the tools that are available through the HAPPy project can be found on the respective tool pages: ADAMS happy-tools Segment Anything Segment Anything in High Quality NB: These pages also contain detailed instructions on how to install them, which you can ignore if you used the happy-setup.sh installation script. Docker # For available Docker images, please see the Docker page.","title":"Introduction"},{"location":"happy/#overview","text":"HAPPy, short for \"Hyperspectral Application Platform in Python,\" stands as a pioneering platform at the forefront of hyperspectral imaging technology. Designed by the Hyperspectral Imaging Group at the University of Waikato, HAPPy unifies hyperspectral workflows for how hyperspectral images are acquired, processed, and analyzed. At its core, HAPPy seamlessly integrates data acquisition, annotation, analysis, and the exploration of novel statistical machine learning and deep learning approaches.","title":"Overview"},{"location":"happy/#key-features","text":"Containerized Algorithm Management: HAPPy simplifies algorithm deployment, ensuring compatibility with open-source machine learning platforms. Hardware-Software Calibration Protocols: Real-time machine learning model assessment at the imaging front enhances adaptability. Efficiency in Hyperspectral Workflow: Accelerated experimentation allows quick feasibility assessment and method shortlisting for development. Empowering Innovation: HAPPy empowers researchers to bridge the gap between hyperspectral data and state-of-the-art machine learning algorithms. It is the tool that fosters quicker experimentation, innovation, and discovery within the dynamic field of hyperspectral imaging. Open Source and Collaborative: Committed to the principles of open-source collaboration, we invite researchers, engineers, and enthusiasts from around the world to explore, contribute, and leverage the HAPPy. Get Started: Whether you're a seasoned hyperspectral imaging expert or just embarking on your journey, HAPPy is here to simplify, accelerate, and amplify your research efforts. Scroll down for information on installation and usage, as well as access to our tools and publications.","title":"Key Features"},{"location":"happy/#installation","text":"The tools have been tested on Linux (Ubuntu/Debian) and under Windows with WSL2 using Ubuntu 22.04.x. Notes on WSL2 Installing the tools","title":"Installation"},{"location":"happy/#usage","text":"Once installed using the above instructions, you can launch graphical tools and Docker images (start/stop) using the happy-launch.sh script from your home directory: ./happy-launch.sh","title":"Usage"},{"location":"happy/#tools","text":"More details on the tools that are available through the HAPPy project can be found on the respective tool pages: ADAMS happy-tools Segment Anything Segment Anything in High Quality NB: These pages also contain detailed instructions on how to install them, which you can ignore if you used the happy-setup.sh installation script.","title":"Tools"},{"location":"happy/#docker","text":"For available Docker images, please see the Docker page.","title":"Docker"},{"location":"happy/adams/","text":"ADAMS-based framework that can be used for annotating images using its powerful workflow engine. Requirements # OpenJDK 11+ Windows: adoptium.net/temurin Debian/Ubuntu: sudo apt install openjdk-11-jdk Installation # Download a snapshot (in ZIP format) adams.cms.waikato.ac.nz/snapshots/happy/ Unzip the ZIP archive and rename the generated directory to happy-adams Starting the application # Start the user interface with: Windows: happy-adams\\bin\\start_gui.bat Linux: happy-adams/bin/start_gui.sh Available flows # Of the flows that are come with the Happy ADAMS framework, the following ones are relevant to the Happy project: adams-imaging-annotate_objects.flow - generating annotations for object detection (bounding box or polygon) adams-imaging-image_segmentation_annotation.flow - generating annotations for image segmentation (pixel-level classification) adams-imaging-ext_run-sam.flow - downloads and runs SAM (Segment Anything Model) via Docker (can be used within the above two flows as a separate annotation tool) Tutorials # Instructions on how to use these flows are available from the Applied Deep Learning website, specifically: object detection image segmentation Preview browser # The Preview browser (from the Visualization menu) can be used for viewing PNG/OPEX JSON files that were export from the happy-envi-viewer tool. Using the ObjectLocationsFromReport with the OpexObjectLocationsReader you can generate an overlay of the annotations like this: The options used can be seen here: And here as a configuration setup that you can paste via the drop-down button in the top-right corner of the options dialog: # Project: adams # Date: 2023-08-23 16:26:05 # User: fracpete # Charset: UTF-8 # Modules: adams-core,adams-docker,adams-imaging,adams-imaging-ext,adams-json,adams-meta,adams-net,adams-redis,adams-spreadsheet,adams-xml # adams.gui.tools.previewbrowser.ObjectLocationsFromReport -image-reader adams.data.io.input.JAIImageReader -reader adams.data.io.input.OpexObjectLocationsReader -type-color-provider adams.gui.visualization.core.DefaultColorProvider -label-anchor MIDDLE_CENTER -shape-color-provider adams.gui.visualization.core.TranslucentColorProvider -provider adams.gui.visualization.core.DefaultColorProvider -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal adams.data.overlappingobjectremoval.PassThrough -show-object-panel true","title":"ADAMS"},{"location":"happy/adams/#requirements","text":"OpenJDK 11+ Windows: adoptium.net/temurin Debian/Ubuntu: sudo apt install openjdk-11-jdk","title":"Requirements"},{"location":"happy/adams/#installation","text":"Download a snapshot (in ZIP format) adams.cms.waikato.ac.nz/snapshots/happy/ Unzip the ZIP archive and rename the generated directory to happy-adams","title":"Installation"},{"location":"happy/adams/#starting-the-application","text":"Start the user interface with: Windows: happy-adams\\bin\\start_gui.bat Linux: happy-adams/bin/start_gui.sh","title":"Starting the application"},{"location":"happy/adams/#available-flows","text":"Of the flows that are come with the Happy ADAMS framework, the following ones are relevant to the Happy project: adams-imaging-annotate_objects.flow - generating annotations for object detection (bounding box or polygon) adams-imaging-image_segmentation_annotation.flow - generating annotations for image segmentation (pixel-level classification) adams-imaging-ext_run-sam.flow - downloads and runs SAM (Segment Anything Model) via Docker (can be used within the above two flows as a separate annotation tool)","title":"Available flows"},{"location":"happy/adams/#tutorials","text":"Instructions on how to use these flows are available from the Applied Deep Learning website, specifically: object detection image segmentation","title":"Tutorials"},{"location":"happy/adams/#preview-browser","text":"The Preview browser (from the Visualization menu) can be used for viewing PNG/OPEX JSON files that were export from the happy-envi-viewer tool. Using the ObjectLocationsFromReport with the OpexObjectLocationsReader you can generate an overlay of the annotations like this: The options used can be seen here: And here as a configuration setup that you can paste via the drop-down button in the top-right corner of the options dialog: # Project: adams # Date: 2023-08-23 16:26:05 # User: fracpete # Charset: UTF-8 # Modules: adams-core,adams-docker,adams-imaging,adams-imaging-ext,adams-json,adams-meta,adams-net,adams-redis,adams-spreadsheet,adams-xml # adams.gui.tools.previewbrowser.ObjectLocationsFromReport -image-reader adams.data.io.input.JAIImageReader -reader adams.data.io.input.OpexObjectLocationsReader -type-color-provider adams.gui.visualization.core.DefaultColorProvider -label-anchor MIDDLE_CENTER -shape-color-provider adams.gui.visualization.core.TranslucentColorProvider -provider adams.gui.visualization.core.DefaultColorProvider -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal adams.data.overlappingobjectremoval.PassThrough -show-object-panel true","title":"Preview browser"},{"location":"happy/docker/","text":"To make it easier to use the libraries (and avoid any hassle with installing libraries and required dependencies), we also make them available as Docker images: happy-tools happy-tools-keras If you are not familiar with Docker, please feel free to have a look at the Docker for Data Scientists introduction.","title":"Docker"},{"location":"happy/installation/","text":"Prerequisites # Linux (Ubuntu/Debian) Docker ( sudo apt install docker.io ) redis-server ( sudo apt install redis-server ) wget ( sudo apt install wget ) Windows/WSL2 Ubuntu 22.04.x from the Microsoft store Docker redis-server ( sudo apt install redis-server ) wget ( sudo apt install wget ) Installation # Go to your home directory: cd ~ Download the happy-setup.sh script and make it executable: wget -O happy-setup.sh https://raw.githubusercontent.com/wairas/happy-scripts/main/happy-setup.sh chmod a+x happy-setup.sh Execute the script: ./happy-setup.sh Minimal installation items to execute: Prepare system (ensures that all required system libraries are present) Install Happy Tools Install SAM-HQ (or if you prefer, Install SAM ) Notes: SAM and SAM-HQ can be installed in parallel, but only one of them can actively running, as they both use the same redis channels for communication (that way they are interchangeable). ADAMS can be used for annotating objects in your scanned images.","title":"Installation"},{"location":"happy/installation/#prerequisites","text":"Linux (Ubuntu/Debian) Docker ( sudo apt install docker.io ) redis-server ( sudo apt install redis-server ) wget ( sudo apt install wget ) Windows/WSL2 Ubuntu 22.04.x from the Microsoft store Docker redis-server ( sudo apt install redis-server ) wget ( sudo apt install wget )","title":"Prerequisites"},{"location":"happy/installation/#installation","text":"Go to your home directory: cd ~ Download the happy-setup.sh script and make it executable: wget -O happy-setup.sh https://raw.githubusercontent.com/wairas/happy-scripts/main/happy-setup.sh chmod a+x happy-setup.sh Execute the script: ./happy-setup.sh Minimal installation items to execute: Prepare system (ensures that all required system libraries are present) Install Happy Tools Install SAM-HQ (or if you prefer, Install SAM ) Notes: SAM and SAM-HQ can be installed in parallel, but only one of them can actively running, as they both use the same redis channels for communication (that way they are interchangeable). ADAMS can be used for annotating objects in your scanned images.","title":"Installation"},{"location":"happy/sam-hq/","text":"Segment Anything in High Quality (SAM-HQ) works like SAM and consists of pretrained models that perform image segmentation on RGB images and can aid the human in the annotation process. Prerequisites # Linux # docker redis-server ( sudo apt install redis-server ) Windows # WSL2 using 22.04.x docker ( instructions ) redis-server ( sudo apt install redis-server ) Directories # sam-hq | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM-HQ models You can create the structure using the following command: mkdir -p sam-hq/cache \\ mkdir -p sam-hq/models Pretrained models # Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam-hq/models directory, run the following command: wget https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_l.pth Service scripts (WSL2 without Docker Desktop UI) # Create a bash script happy_samhq_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_samhq_start.sh Create a bash script happy_samhq_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happyhq_sam_stop.sh SAM scripts # In the sam-hq directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam-hq:2023-08-17_cuda11.6 \\ samhq_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_hq_vit_l.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . NB: This script uses the sam_in and sam_out Redis channels to make it a drop-in replacement for SAM in the happy-envi-viewer . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]amhq_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh . Starting # Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_samhq_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background. SAM # In the sam-hq directory, execute the start.sh script. Stopping # SAM # In the sam-hq directory, execute the stop.sh script. Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_samhq_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"SAM-HQ"},{"location":"happy/sam-hq/#prerequisites","text":"","title":"Prerequisites"},{"location":"happy/sam-hq/#linux","text":"docker redis-server ( sudo apt install redis-server )","title":"Linux"},{"location":"happy/sam-hq/#windows","text":"WSL2 using 22.04.x docker ( instructions ) redis-server ( sudo apt install redis-server )","title":"Windows"},{"location":"happy/sam-hq/#directories","text":"sam-hq | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM-HQ models You can create the structure using the following command: mkdir -p sam-hq/cache \\ mkdir -p sam-hq/models","title":"Directories"},{"location":"happy/sam-hq/#pretrained-models","text":"Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam-hq/models directory, run the following command: wget https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_l.pth","title":"Pretrained models"},{"location":"happy/sam-hq/#service-scripts-wsl2-without-docker-desktop-ui","text":"Create a bash script happy_samhq_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_samhq_start.sh Create a bash script happy_samhq_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happyhq_sam_stop.sh","title":"Service scripts (WSL2 without Docker Desktop UI)"},{"location":"happy/sam-hq/#sam-scripts","text":"In the sam-hq directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam-hq:2023-08-17_cuda11.6 \\ samhq_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_hq_vit_l.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . NB: This script uses the sam_in and sam_out Redis channels to make it a drop-in replacement for SAM in the happy-envi-viewer . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]amhq_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh .","title":"SAM scripts"},{"location":"happy/sam-hq/#starting","text":"","title":"Starting"},{"location":"happy/sam-hq/#docker-and-redis-wsl2-without-docker-desktop-ui","text":"sudo /usr/local/bin/happy_samhq_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/sam-hq/#sam","text":"In the sam-hq directory, execute the start.sh script.","title":"SAM"},{"location":"happy/sam-hq/#stopping","text":"","title":"Stopping"},{"location":"happy/sam-hq/#sam_1","text":"In the sam-hq directory, execute the stop.sh script.","title":"SAM"},{"location":"happy/sam-hq/#docker-and-redis-wsl2-without-docker-desktop-ui_1","text":"sudo /usr/local/bin/happy_samhq_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/","text":"Facebook's Segment Anything (SAM) are pretrained models that perform image segmentation on RGB images and can aid the human in the annotation process. Prerequisites # Linux # docker redis-server ( sudo apt install redis-server ) Windows # WSL2 using 22.04.x docker ( instructions ) redis-server ( sudo apt install redis-server ) Directories # sam | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM models You can create the structure using the following command: mkdir -p sam/cache \\ mkdir -p sam/models Pretrained models # Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam/models directory, run the following command: wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth Service scripts (WSL2 without Docker Desktop UI) # Create a bash script happy_sam_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_sam_start.sh Create a bash script happy_sam_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happy_sam_stop.sh SAM scripts # In the sam directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam:2023-04-16_cuda11.6 \\ sam_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_vit_l_0b3195.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]am_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh . Starting # Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_sam_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background. SAM # In the sam directory, execute the start.sh script. Stopping # SAM # In the sam directory, execute the stop.sh script. Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_sam_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"SAM"},{"location":"happy/sam/#prerequisites","text":"","title":"Prerequisites"},{"location":"happy/sam/#linux","text":"docker redis-server ( sudo apt install redis-server )","title":"Linux"},{"location":"happy/sam/#windows","text":"WSL2 using 22.04.x docker ( instructions ) redis-server ( sudo apt install redis-server )","title":"Windows"},{"location":"happy/sam/#directories","text":"sam | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM models You can create the structure using the following command: mkdir -p sam/cache \\ mkdir -p sam/models","title":"Directories"},{"location":"happy/sam/#pretrained-models","text":"Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam/models directory, run the following command: wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth","title":"Pretrained models"},{"location":"happy/sam/#service-scripts-wsl2-without-docker-desktop-ui","text":"Create a bash script happy_sam_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_sam_start.sh Create a bash script happy_sam_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happy_sam_stop.sh","title":"Service scripts (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/#sam-scripts","text":"In the sam directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam:2023-04-16_cuda11.6 \\ sam_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_vit_l_0b3195.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]am_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh .","title":"SAM scripts"},{"location":"happy/sam/#starting","text":"","title":"Starting"},{"location":"happy/sam/#docker-and-redis-wsl2-without-docker-desktop-ui","text":"sudo /usr/local/bin/happy_sam_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/#sam","text":"In the sam directory, execute the start.sh script.","title":"SAM"},{"location":"happy/sam/#stopping","text":"","title":"Stopping"},{"location":"happy/sam/#sam_1","text":"In the sam directory, execute the stop.sh script.","title":"SAM"},{"location":"happy/sam/#docker-and-redis-wsl2-without-docker-desktop-ui_1","text":"sudo /usr/local/bin/happy_sam_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/wsl2/","text":"Though all the tools are working under the Windows Subsystem for Linux , Docker and graphical applications only work as long as you are using version 2 of WSL and your Windows 10 is at least build 19044 (no restrictions with Windows 11) . NB: The following commands are all to be issued from a Windows command-prompt, not a Linux shell. First, make sure that your WSL is up-to-date (and supports graphical applications): wsl --update Here is how to switch WSL to version 2 as default by running the following command: wsl --set-default-version 2 You can view all your current images and what WSL version they are using with the following command: wsl --list -v If you want to switch an existing image (e.g., Ubuntu-22.04 ) to version 2, then you can run the following command: wsl --set-version Ubuntu-22.04 2 Source: https://stackoverflow.com/a/73164601 Advanced configuration # For more information on how to configure WSL2, please see here .","title":"WSL2"},{"location":"happy/wsl2/#advanced-configuration","text":"For more information on how to configure WSL2, please see here .","title":"Advanced configuration"},{"location":"happy/happy_tools/","text":"happy-tools contains several command-line utilities and graphical viewers for HSI files: happy-data-viewer - for viewing HAPPy data folders happy-envi-viewer - for viewing ENVI HSI images happy-generate-image-regions-objects - generates datasets as numpy cubes for deep learning happy-hdr-info - outputs information on ENVI HDR files happy-hsi2rbg - generates fake RGB PNG files from HSI images happy-mat-info - outputs Matlab struct information happy-opex2happy - converts OPEX JSON annotations and PNG images into happy data structures happy-process-data - applies a preprocessing pipeline to the data happy-plot-preproc - plots set of pixels using various preprocessors happy-scikit-regression-build - evaluates regression models on HAPPy data happy-scikit-unsupervised-build - evaluates cluster models on HAPPy data happy-splitter - for generating train/validation/test splits for HAPPy data These tools are available from the Python virtual environment that they were installed. E.g., when following the installation instructions on this website, the tools would be located in the following directory in the user's home folder: happy/bin","title":"Overview"},{"location":"happy/happy_tools/happy-data-viewer/","text":"Used for viewing data that has been converted into a HAPPy folder structure. Command-line # usage: happy-data-viewer [-h] [--base_folder BASE_FOLDER] [--sample SAMPLE] [--repeat REPEAT] [-r INT] [-g INT] [-b INT] [-o INT] [--listbox_selectbackground LISTBOX_SELECTBACKGROUND] [--listbox_selectforeground LISTBOX_SELECTFOREGROUND] [--log_timestamp_format FORMAT] Viewer for HAPPy data folder structures. optional arguments: -h, --help show this help message and exit --base_folder BASE_FOLDER Base folder to display (default: None) --sample SAMPLE The sample to load (default: None) --repeat REPEAT The repeat to load (default: None) -r INT, --scale_r INT the wave length to use for the red channel (default: None) -g INT, --scale_g INT the wave length to use for the green channel (default: None) -b INT, --scale_b INT the wave length to use for the blue channel (default: None) -o INT, --opacity INT the opacity to use (0-100) (default: None) --listbox_selectbackground LISTBOX_SELECTBACKGROUND The background color to use for selected items in listboxes (default: #4a6984) --listbox_selectforeground LISTBOX_SELECTFOREGROUND The foreground color to use for selected items in listboxes (default: #ffffff) --log_timestamp_format FORMAT the format string for the logging timestamp, see: http s://docs.python.org/3/library/datetime.html#strftime- and-strptime-format-codes (default: [%H:%M:%S.%f])","title":"happy-data-viewer"},{"location":"happy/happy_tools/happy-data-viewer/#command-line","text":"usage: happy-data-viewer [-h] [--base_folder BASE_FOLDER] [--sample SAMPLE] [--repeat REPEAT] [-r INT] [-g INT] [-b INT] [-o INT] [--listbox_selectbackground LISTBOX_SELECTBACKGROUND] [--listbox_selectforeground LISTBOX_SELECTFOREGROUND] [--log_timestamp_format FORMAT] Viewer for HAPPy data folder structures. optional arguments: -h, --help show this help message and exit --base_folder BASE_FOLDER Base folder to display (default: None) --sample SAMPLE The sample to load (default: None) --repeat REPEAT The repeat to load (default: None) -r INT, --scale_r INT the wave length to use for the red channel (default: None) -g INT, --scale_g INT the wave length to use for the green channel (default: None) -b INT, --scale_b INT the wave length to use for the blue channel (default: None) -o INT, --opacity INT the opacity to use (0-100) (default: None) --listbox_selectbackground LISTBOX_SELECTBACKGROUND The background color to use for selected items in listboxes (default: #4a6984) --listbox_selectforeground LISTBOX_SELECTFOREGROUND The foreground color to use for selected items in listboxes (default: #ffffff) --log_timestamp_format FORMAT the format string for the logging timestamp, see: http s://docs.python.org/3/library/datetime.html#strftime- and-strptime-format-codes (default: [%H:%M:%S.%f])","title":"Command-line"},{"location":"happy/happy_tools/happy-envi-viewer/","text":"Usage # Image tab # Via the File menu, you can load an ENVI file representing a sample scan. From that menu, you can also select black and white reference ENVI files that get automatically applied to the scan: At the bottom of the window, you get a quick info on what dimensions the scan has (width, height and channels). The three sliders allow you to select the channels from the hyperspectral image to act as red, green and blue channel for the fake RGB image that is being displayed. Left-clicking on the label next to the slider, depicting the current channel value, pops up a dialog for entering a specific channel. If default bands are defined in the ENVI header and auto-detect channels is enabled on the Options tab , then these will get used when loading the file. Info tab # On the Info tab, you can see what files are currently loaded and what dimensions these files have: Options tab # On the Options tab, you can change various view settings, how to connect to SAM and how annotations appear: Annotations # The envi-viewer also allows you to annotate images and then export them. The image will be exported as PNG using the currently selected channels. Any annotations present will get exported as JSON, using the OPEX format . Such annotations in OPEX format can be viewed in the ADAMS Preview browser . General mouse usage: Left-clicking on the image sets a marker point. Left-clicking while holding the CTRL key removes any marker points. Left-clicking on an existing annotation shape while holding the SHIFT key allows you to enter a label for that shape (e.g., white_ref or leaf ). Tools menu: Clear annotations - removes any annotations Clear markers - removes all marker points Remove last annotation - removes the most recent annotation that was added (can be repeated till there are no more annotations) Polygon - turns current marker points into polygon SAM - uses current marker points as prompt points for SAM Polygons # The simplest way of annotating that does not require any further tools is by using polygons. First define the outline of the object with marker points: Once at least three marker points have been put on the image, selecting Polygon from the Tools menu turns them into a polygon annotation: SAM # Using SAM , you can easily annotate complex shapes accurately. Though SAM can run on a CPU, it is recommended to use a computer with a NVIDIA GPU as it will speed up the detection process by at least 10 times. SAM requires you to at least provide a single marker on the object that you want to trace the shape for. Depending on the object and how well it is separated from the background, how much the colors on the object change, you may have to provide more than one marker point to better guide the detection: The result looks then like this: Command-line # Using the command-line options, you can preset the options in the user interface and also load scan, black and white reference files: usage: happy-envi-viewer [-h] [-s SCAN] [-f BLACK_REFERENCE] [-w WHITE_REFERENCE] [-r INT] [-g INT] [-b INT] [--autodetect_channels] [--keep_aspectratio] [--check_scan_dimensions] [--export_to_scan_dir] [--annotation_color HEXCOLOR] [--redis_host HOST] [--redis_port PORT] [--redis_pw PASSWORD] [--redis_in CHANNEL] [--redis_out CHANNEL] [--redis_connect] [--marker_size INT] [--marker_color HEXCOLOR] [--min_obj_size INT] [--black_ref_locator LOCATOR] [--black_ref_method METHOD] [--white_ref_locator LOCATOR] [--white_ref_method METHOD] [--preprocessing PIPELINE] [--log_timestamp_format FORMAT] ENVI Hyperspectral Image Viewer. Offers contour detection using SAM (Segment- Anything: https://github.com/waikato-datamining/pytorch/tree/master/segment- anything) optional arguments: -h, --help show this help message and exit -s SCAN, --scan SCAN Path to the scan file (ENVI format) (default: None) -f BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -r INT, --scale_r INT the wave length to use for the red channel (default: None) -g INT, --scale_g INT the wave length to use for the green channel (default: None) -b INT, --scale_b INT the wave length to use for the blue channel (default: None) --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: None) --keep_aspectratio whether to keep the aspect ratio (default: None) --check_scan_dimensions whether to compare the dimensions of subsequently loaded scans and output a warning if they differ (default: None) --export_to_scan_dir whether to export images to the scan directory rather than the last one used (default: None) --annotation_color HEXCOLOR the color to use for the annotations like contours (hex color) (default: None) --redis_host HOST The Redis host to connect to (IP or hostname) (default: None) --redis_port PORT The port the Redis server is listening on (default: None) --redis_pw PASSWORD The password to use with the Redis server (default: None) --redis_in CHANNEL The channel that SAM is receiving images on (default: None) --redis_out CHANNEL The channel that SAM is broadcasting the detections on (default: None) --redis_connect whether to immediately connect to the Redis host (default: None) --marker_size INT The size in pixels for the SAM points (default: None) --marker_color HEXCOLOR the color to use for the SAM points (hex color) (default: None) --min_obj_size INT The minimum size that SAM contours need to have (<= 0 for no minimum) (default: None) --black_ref_locator LOCATOR the reference locator scheme to use for locating black references, eg rl-manual (default: None) --black_ref_method METHOD the black reference method to use for applying black references, eg br-same-size (default: None) --white_ref_locator LOCATOR the reference locator scheme to use for locating whites references, eg rl-manual (default: None) --white_ref_method METHOD the white reference method to use for applying white references, eg wr-same-size (default: None) --preprocessing PIPELINE the preprocessors to apply to the scan (default: None) --log_timestamp_format FORMAT the format string for the logging timestamp, see: http s://docs.python.org/3/library/datetime.html#strftime- and-strptime-format-codes (default: [%H:%M:%S.%f])","title":"happy-envi-viewer"},{"location":"happy/happy_tools/happy-envi-viewer/#usage","text":"","title":"Usage"},{"location":"happy/happy_tools/happy-envi-viewer/#image-tab","text":"Via the File menu, you can load an ENVI file representing a sample scan. From that menu, you can also select black and white reference ENVI files that get automatically applied to the scan: At the bottom of the window, you get a quick info on what dimensions the scan has (width, height and channels). The three sliders allow you to select the channels from the hyperspectral image to act as red, green and blue channel for the fake RGB image that is being displayed. Left-clicking on the label next to the slider, depicting the current channel value, pops up a dialog for entering a specific channel. If default bands are defined in the ENVI header and auto-detect channels is enabled on the Options tab , then these will get used when loading the file.","title":"Image tab"},{"location":"happy/happy_tools/happy-envi-viewer/#info-tab","text":"On the Info tab, you can see what files are currently loaded and what dimensions these files have:","title":"Info tab"},{"location":"happy/happy_tools/happy-envi-viewer/#options-tab","text":"On the Options tab, you can change various view settings, how to connect to SAM and how annotations appear:","title":"Options tab"},{"location":"happy/happy_tools/happy-envi-viewer/#annotations","text":"The envi-viewer also allows you to annotate images and then export them. The image will be exported as PNG using the currently selected channels. Any annotations present will get exported as JSON, using the OPEX format . Such annotations in OPEX format can be viewed in the ADAMS Preview browser . General mouse usage: Left-clicking on the image sets a marker point. Left-clicking while holding the CTRL key removes any marker points. Left-clicking on an existing annotation shape while holding the SHIFT key allows you to enter a label for that shape (e.g., white_ref or leaf ). Tools menu: Clear annotations - removes any annotations Clear markers - removes all marker points Remove last annotation - removes the most recent annotation that was added (can be repeated till there are no more annotations) Polygon - turns current marker points into polygon SAM - uses current marker points as prompt points for SAM","title":"Annotations"},{"location":"happy/happy_tools/happy-envi-viewer/#polygons","text":"The simplest way of annotating that does not require any further tools is by using polygons. First define the outline of the object with marker points: Once at least three marker points have been put on the image, selecting Polygon from the Tools menu turns them into a polygon annotation:","title":"Polygons"},{"location":"happy/happy_tools/happy-envi-viewer/#sam","text":"Using SAM , you can easily annotate complex shapes accurately. Though SAM can run on a CPU, it is recommended to use a computer with a NVIDIA GPU as it will speed up the detection process by at least 10 times. SAM requires you to at least provide a single marker on the object that you want to trace the shape for. Depending on the object and how well it is separated from the background, how much the colors on the object change, you may have to provide more than one marker point to better guide the detection: The result looks then like this:","title":"SAM"},{"location":"happy/happy_tools/happy-envi-viewer/#command-line","text":"Using the command-line options, you can preset the options in the user interface and also load scan, black and white reference files: usage: happy-envi-viewer [-h] [-s SCAN] [-f BLACK_REFERENCE] [-w WHITE_REFERENCE] [-r INT] [-g INT] [-b INT] [--autodetect_channels] [--keep_aspectratio] [--check_scan_dimensions] [--export_to_scan_dir] [--annotation_color HEXCOLOR] [--redis_host HOST] [--redis_port PORT] [--redis_pw PASSWORD] [--redis_in CHANNEL] [--redis_out CHANNEL] [--redis_connect] [--marker_size INT] [--marker_color HEXCOLOR] [--min_obj_size INT] [--black_ref_locator LOCATOR] [--black_ref_method METHOD] [--white_ref_locator LOCATOR] [--white_ref_method METHOD] [--preprocessing PIPELINE] [--log_timestamp_format FORMAT] ENVI Hyperspectral Image Viewer. Offers contour detection using SAM (Segment- Anything: https://github.com/waikato-datamining/pytorch/tree/master/segment- anything) optional arguments: -h, --help show this help message and exit -s SCAN, --scan SCAN Path to the scan file (ENVI format) (default: None) -f BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -r INT, --scale_r INT the wave length to use for the red channel (default: None) -g INT, --scale_g INT the wave length to use for the green channel (default: None) -b INT, --scale_b INT the wave length to use for the blue channel (default: None) --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: None) --keep_aspectratio whether to keep the aspect ratio (default: None) --check_scan_dimensions whether to compare the dimensions of subsequently loaded scans and output a warning if they differ (default: None) --export_to_scan_dir whether to export images to the scan directory rather than the last one used (default: None) --annotation_color HEXCOLOR the color to use for the annotations like contours (hex color) (default: None) --redis_host HOST The Redis host to connect to (IP or hostname) (default: None) --redis_port PORT The port the Redis server is listening on (default: None) --redis_pw PASSWORD The password to use with the Redis server (default: None) --redis_in CHANNEL The channel that SAM is receiving images on (default: None) --redis_out CHANNEL The channel that SAM is broadcasting the detections on (default: None) --redis_connect whether to immediately connect to the Redis host (default: None) --marker_size INT The size in pixels for the SAM points (default: None) --marker_color HEXCOLOR the color to use for the SAM points (hex color) (default: None) --min_obj_size INT The minimum size that SAM contours need to have (<= 0 for no minimum) (default: None) --black_ref_locator LOCATOR the reference locator scheme to use for locating black references, eg rl-manual (default: None) --black_ref_method METHOD the black reference method to use for applying black references, eg br-same-size (default: None) --white_ref_locator LOCATOR the reference locator scheme to use for locating whites references, eg rl-manual (default: None) --white_ref_method METHOD the white reference method to use for applying white references, eg wr-same-size (default: None) --preprocessing PIPELINE the preprocessors to apply to the scan (default: None) --log_timestamp_format FORMAT the format string for the logging timestamp, see: http s://docs.python.org/3/library/datetime.html#strftime- and-strptime-format-codes (default: [%H:%M:%S.%f])","title":"Command-line"},{"location":"happy/happy_tools/happy-generate-image-regions-objects/","text":"Command-line # usage: happy-generate-image-regions-objects [-h] -i INPUT_DIR -o OUTPUT_DIR Generate datasets as numpy cubes, to be loaded into deep learning datasets. optional arguments: -h, --help show this help message and exit -i INPUT_DIR, --input_dir INPUT_DIR Path to source folder containing HDR files (default: None) -o OUTPUT_DIR, --output_dir OUTPUT_DIR Path to output folder (default: None)","title":"Command-line"},{"location":"happy/happy_tools/happy-generate-image-regions-objects/#command-line","text":"usage: happy-generate-image-regions-objects [-h] -i INPUT_DIR -o OUTPUT_DIR Generate datasets as numpy cubes, to be loaded into deep learning datasets. optional arguments: -h, --help show this help message and exit -i INPUT_DIR, --input_dir INPUT_DIR Path to source folder containing HDR files (default: None) -o OUTPUT_DIR, --output_dir OUTPUT_DIR Path to output folder (default: None)","title":"Command-line"},{"location":"happy/happy_tools/happy-hdr-info/","text":"Command-line # usage: happy-hdr-info [-h] -i INPUT_FILE [-o OUTPUT_FILE] Load and print information about an HDR file. optional arguments: -h, --help show this help message and exit -i INPUT_FILE, --input_file INPUT_FILE Path to the HDR file (default: None) -o OUTPUT_FILE, --output_file OUTPUT_FILE Path to output file; prints to stdout if omitted (default: None)","title":"happy-hdr-info"},{"location":"happy/happy_tools/happy-hdr-info/#command-line","text":"usage: happy-hdr-info [-h] -i INPUT_FILE [-o OUTPUT_FILE] Load and print information about an HDR file. optional arguments: -h, --help show this help message and exit -i INPUT_FILE, --input_file INPUT_FILE Path to the HDR file (default: None) -o OUTPUT_FILE, --output_file OUTPUT_FILE Path to output file; prints to stdout if omitted (default: None)","title":"Command-line"},{"location":"happy/happy_tools/happy-hsi2rbg/","text":"Command-line # usage: happy-hsi2rgb [-h] -i INPUT_DIR [INPUT_DIR ...] [-r] [-e EXTENSION] [-b BLACK_REFERENCE] [-w WHITE_REFERENCE] [-a] [--red INT] [--green INT] [--blue INT] [-o OUTPUT_DIR] [--width INT] [--height INT] [-n] [-v] Fake RGB image generator for HSI files. optional arguments: -h, --help show this help message and exit -i INPUT_DIR [INPUT_DIR ...], --input_dir INPUT_DIR [INPUT_DIR ...] Path to the scan file (ENVI format) (default: None) -r, --recursive whether to traverse the directories recursively (default: False) -e EXTENSION, --extension EXTENSION The file extension to look for (default: .hdr) -b BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -a, --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: False) --red INT the wave length to use for the red channel (0-based) (default: 0) --green INT the wave length to use for the green channel (0-based) (default: 0) --blue INT the wave length to use for the blue channel (0-based) (default: 0) -o OUTPUT_DIR, --output_dir OUTPUT_DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) --width INT the width to scale the images to (<= 0 uses image dimension) (default: 0) --height INT the height to scale the images to (<= 0 uses image dimension) (default: 0) -n, --dry_run whether to omit saving the PNG images (default: False) -v, --verbose whether to be more verbose with the output (default: False)","title":"happy-hsi2rbg"},{"location":"happy/happy_tools/happy-hsi2rbg/#command-line","text":"usage: happy-hsi2rgb [-h] -i INPUT_DIR [INPUT_DIR ...] [-r] [-e EXTENSION] [-b BLACK_REFERENCE] [-w WHITE_REFERENCE] [-a] [--red INT] [--green INT] [--blue INT] [-o OUTPUT_DIR] [--width INT] [--height INT] [-n] [-v] Fake RGB image generator for HSI files. optional arguments: -h, --help show this help message and exit -i INPUT_DIR [INPUT_DIR ...], --input_dir INPUT_DIR [INPUT_DIR ...] Path to the scan file (ENVI format) (default: None) -r, --recursive whether to traverse the directories recursively (default: False) -e EXTENSION, --extension EXTENSION The file extension to look for (default: .hdr) -b BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -a, --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: False) --red INT the wave length to use for the red channel (0-based) (default: 0) --green INT the wave length to use for the green channel (0-based) (default: 0) --blue INT the wave length to use for the blue channel (0-based) (default: 0) -o OUTPUT_DIR, --output_dir OUTPUT_DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) --width INT the width to scale the images to (<= 0 uses image dimension) (default: 0) --height INT the height to scale the images to (<= 0 uses image dimension) (default: 0) -n, --dry_run whether to omit saving the PNG images (default: False) -v, --verbose whether to be more verbose with the output (default: False)","title":"Command-line"},{"location":"happy/happy_tools/happy-mat-info/","text":"Command-line # usage: happy-mat-info [-h] -i INPUT_FILE [-o OUTPUT_FILE] Load and display structs from a MATLAB file. optional arguments: -h, --help show this help message and exit -i INPUT_FILE, --input_file INPUT_FILE Path to the MATLAB file (default: None) -o OUTPUT_FILE, --output_file OUTPUT_FILE Path to the output file; outputs to stdout if omitted (default: None)","title":"happy-mat-info"},{"location":"happy/happy_tools/happy-mat-info/#command-line","text":"usage: happy-mat-info [-h] -i INPUT_FILE [-o OUTPUT_FILE] Load and display structs from a MATLAB file. optional arguments: -h, --help show this help message and exit -i INPUT_FILE, --input_file INPUT_FILE Path to the MATLAB file (default: None) -o OUTPUT_FILE, --output_file OUTPUT_FILE Path to the output file; outputs to stdout if omitted (default: None)","title":"Command-line"},{"location":"happy/happy_tools/happy-opex2happy/","text":"Command-line # usage: happy-opex2happy [-h] -i DIR [DIR ...] [-r] [-o DIR] -f {flat,dir-tree,dir-tree-with-data} -l LABELS [--black_ref_locator LOCATOR] [--black_ref_method METHOD] [--white_ref_locator LOCATOR] [--white_ref_method METHOD] [--pattern_mask PATTERN] [--pattern_labels PATTERN] [--pattern_png PATTERN] [--pattern_annotations PATTERN] [-I] [-n] [-v] Turns annotations (PNG and OPEX JSON) into Happy ENVI format. optional arguments: -h, --help show this help message and exit -i DIR [DIR ...], --input_dir DIR [DIR ...] Path to the PNG/OPEX files (default: None) -r, --recursive whether to look for OPEX files recursively (default: False) -o DIR, --output_dir DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) -f {flat,dir-tree,dir-tree-with-data}, --output_format {flat,dir-tree,dir-tree-with-data} Defines how to store the data in the output directory. (default: dir-tree-with-data) -l LABELS, --labels LABELS The comma-separated list of object labels to export ('Background' is automatically added). (default: None) --black_ref_locator LOCATOR the reference locator scheme to use for locating black references, eg rl-manual; requires: dir-tree-with-data (default: None) --black_ref_method METHOD the black reference method to use for applying black references, eg br-same-size; requires: dir-tree-with- data (default: None) --white_ref_locator LOCATOR the reference locator scheme to use for locating whites references, eg rl-manual; requires: dir-tree- with-data (default: None) --white_ref_method METHOD the white reference method to use for applying white references, eg wr-same-size; requires: dir-tree-with- data (default: None) --pattern_mask PATTERN the pattern to use for saving the mask ENVI file, available placeholders: {SAMPLEID} (default: mask.hdr) --pattern_labels PATTERN the pattern to use for saving the label map for the mask ENVI file, available placeholders: {SAMPLEID} (default: mask.json) --pattern_png PATTERN the pattern to use for saving the mask PNG file, available placeholders: {SAMPLEID} (default: {SAMPLEID}.png) --pattern_annotations PATTERN the pattern to use for saving the OPEX JSON annotation file, available placeholders: {SAMPLEID} (default: {SAMPLEID}.json) -I, --include_input whether to copy the PNG/JSON file across to the output dir (default: False) -n, --dry_run whether to omit generating any data or creating directories (default: False) -v, --verbose whether to be more verbose with the output (default: False)","title":"happy-opex2happy"},{"location":"happy/happy_tools/happy-opex2happy/#command-line","text":"usage: happy-opex2happy [-h] -i DIR [DIR ...] [-r] [-o DIR] -f {flat,dir-tree,dir-tree-with-data} -l LABELS [--black_ref_locator LOCATOR] [--black_ref_method METHOD] [--white_ref_locator LOCATOR] [--white_ref_method METHOD] [--pattern_mask PATTERN] [--pattern_labels PATTERN] [--pattern_png PATTERN] [--pattern_annotations PATTERN] [-I] [-n] [-v] Turns annotations (PNG and OPEX JSON) into Happy ENVI format. optional arguments: -h, --help show this help message and exit -i DIR [DIR ...], --input_dir DIR [DIR ...] Path to the PNG/OPEX files (default: None) -r, --recursive whether to look for OPEX files recursively (default: False) -o DIR, --output_dir DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) -f {flat,dir-tree,dir-tree-with-data}, --output_format {flat,dir-tree,dir-tree-with-data} Defines how to store the data in the output directory. (default: dir-tree-with-data) -l LABELS, --labels LABELS The comma-separated list of object labels to export ('Background' is automatically added). (default: None) --black_ref_locator LOCATOR the reference locator scheme to use for locating black references, eg rl-manual; requires: dir-tree-with-data (default: None) --black_ref_method METHOD the black reference method to use for applying black references, eg br-same-size; requires: dir-tree-with- data (default: None) --white_ref_locator LOCATOR the reference locator scheme to use for locating whites references, eg rl-manual; requires: dir-tree- with-data (default: None) --white_ref_method METHOD the white reference method to use for applying white references, eg wr-same-size; requires: dir-tree-with- data (default: None) --pattern_mask PATTERN the pattern to use for saving the mask ENVI file, available placeholders: {SAMPLEID} (default: mask.hdr) --pattern_labels PATTERN the pattern to use for saving the label map for the mask ENVI file, available placeholders: {SAMPLEID} (default: mask.json) --pattern_png PATTERN the pattern to use for saving the mask PNG file, available placeholders: {SAMPLEID} (default: {SAMPLEID}.png) --pattern_annotations PATTERN the pattern to use for saving the OPEX JSON annotation file, available placeholders: {SAMPLEID} (default: {SAMPLEID}.json) -I, --include_input whether to copy the PNG/JSON file across to the output dir (default: False) -n, --dry_run whether to omit generating any data or creating directories (default: False) -v, --verbose whether to be more verbose with the output (default: False)","title":"Command-line"},{"location":"happy/happy_tools/happy-plot-preproc/","text":"Command-line # usage: happy-plot-preproc [-h] -i INPUT_DIR [-f FROM_INDEX] [-t TO_INDEX] [-P PREPROCESSORS] [-S PIXEL_SELECTORS] Plot set of pixels with various pre-processing setups. optional arguments: -h, --help show this help message and exit -i INPUT_DIR, --input_dir INPUT_DIR Folder containing HAPPy data files (default: None) -f FROM_INDEX, --from_index FROM_INDEX The first wavelength index to include (0-based) (default: 60) -t TO_INDEX, --to_index TO_INDEX The last wavelength index to include (0-based) (default: 189) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data separately; use \"multi-pp\" if you need to combine multiple steps (default: pass-through multi-pp -p 'derivative -w 15 -d 0 snv' derivative -w 15 -d 0 sni) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. (default: simple-ps -n 100 -b)","title":"happy-plot-preproc"},{"location":"happy/happy_tools/happy-plot-preproc/#command-line","text":"usage: happy-plot-preproc [-h] -i INPUT_DIR [-f FROM_INDEX] [-t TO_INDEX] [-P PREPROCESSORS] [-S PIXEL_SELECTORS] Plot set of pixels with various pre-processing setups. optional arguments: -h, --help show this help message and exit -i INPUT_DIR, --input_dir INPUT_DIR Folder containing HAPPy data files (default: None) -f FROM_INDEX, --from_index FROM_INDEX The first wavelength index to include (0-based) (default: 60) -t TO_INDEX, --to_index TO_INDEX The last wavelength index to include (0-based) (default: 189) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data separately; use \"multi-pp\" if you need to combine multiple steps (default: pass-through multi-pp -p 'derivative -w 15 -d 0 snv' derivative -w 15 -d 0 sni) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. (default: simple-ps -n 100 -b)","title":"Command-line"},{"location":"happy/happy_tools/happy-process-data/","text":"Command-line # usage: happy-process-data reader [preprocessor(s)] writer [-h|--help|--help-all|--help-plugin NAME] Processes data using the specified pipeline. optional arguments: -h, --help show this help message and exit --help-all show the help for all plugins and exit --help-plugin NAME show the help for plugin NAME and exit","title":"happy-process-data"},{"location":"happy/happy_tools/happy-process-data/#command-line","text":"usage: happy-process-data reader [preprocessor(s)] writer [-h|--help|--help-all|--help-plugin NAME] Processes data using the specified pipeline. optional arguments: -h, --help show this help message and exit --help-all show the help for all plugins and exit --help-plugin NAME show the help for plugin NAME and exit","title":"Command-line"},{"location":"happy/happy_tools/happy-scikit-regression-build/","text":"Command-line # usage: happy-scikit-regression-build [-h] -d HAPPY_DATA_BASE_DIR [-P PREPROCESSORS] [-S PIXEL_SELECTORS] [-m REGRESSION_METHOD] [-p REGRESSION_PARAMS] -t TARGET_VALUE -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-r REPEAT_NUM] Evaluate regression model on Happy Data using specified splits and pixel selector. optional arguments: -h, --help show this help message and exit -d HAPPY_DATA_BASE_DIR, --happy_data_base_dir HAPPY_DATA_BASE_DIR Directory containing the Happy Data files (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 sni snv derivative -w 15 pad -W 128 -H 128 -v 0) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. Either pixel selector command-line(s) or file with one pixel selector command-line per line. (default: simple-ps -n 64) -m REGRESSION_METHOD, --regression_method REGRESSION_METHOD Regression method name (e.g., linearregression,ridge,l ars,plsregression,plsneighbourregression,lasso,elastic net,decisiontreeregressor,randomforestregressor,svr or full class name) (default: linearregression) -p REGRESSION_PARAMS, --regression_params REGRESSION_PARAMS JSON string containing regression parameters (default: {}) -t TARGET_VALUE, --target_value TARGET_VALUE Target value column name (default: None) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Happy Splitter file (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Output JSON file to store the predictions (default: None) -r REPEAT_NUM, --repeat_num REPEAT_NUM Repeat number (default: 0) (default: 0)","title":"happy-scikit-regression-build"},{"location":"happy/happy_tools/happy-scikit-regression-build/#command-line","text":"usage: happy-scikit-regression-build [-h] -d HAPPY_DATA_BASE_DIR [-P PREPROCESSORS] [-S PIXEL_SELECTORS] [-m REGRESSION_METHOD] [-p REGRESSION_PARAMS] -t TARGET_VALUE -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-r REPEAT_NUM] Evaluate regression model on Happy Data using specified splits and pixel selector. optional arguments: -h, --help show this help message and exit -d HAPPY_DATA_BASE_DIR, --happy_data_base_dir HAPPY_DATA_BASE_DIR Directory containing the Happy Data files (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 sni snv derivative -w 15 pad -W 128 -H 128 -v 0) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. Either pixel selector command-line(s) or file with one pixel selector command-line per line. (default: simple-ps -n 64) -m REGRESSION_METHOD, --regression_method REGRESSION_METHOD Regression method name (e.g., linearregression,ridge,l ars,plsregression,plsneighbourregression,lasso,elastic net,decisiontreeregressor,randomforestregressor,svr or full class name) (default: linearregression) -p REGRESSION_PARAMS, --regression_params REGRESSION_PARAMS JSON string containing regression parameters (default: {}) -t TARGET_VALUE, --target_value TARGET_VALUE Target value column name (default: None) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Happy Splitter file (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Output JSON file to store the predictions (default: None) -r REPEAT_NUM, --repeat_num REPEAT_NUM Repeat number (default: 0) (default: 0)","title":"Command-line"},{"location":"happy/happy_tools/happy-scikit-unsupervised-build/","text":"Command-line # usage: happy-scikit-unsupervised-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] [-S PIXEL_SELECTORS] [-m CLUSTERER_METHOD] [-p CLUSTERER_PARAMS] -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-r REPEAT_NUM] Evaluate clustering on hyperspectral data using specified clusterer and pixel selector. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Directory containing the hyperspectral data (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 snv derivative pca -n 5 -p 20) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. Either pixel selector command-line(s) or file with one pixel selector command-line per line. (default: simple-ps -n 32 -b) -m CLUSTERER_METHOD, --clusterer_method CLUSTERER_METHOD Clusterer name (e.g., kmeans,agglomerative,spectral,dbscan,meanshift) or full class name (default: kmeans) -p CLUSTERER_PARAMS, --clusterer_params CLUSTERER_PARAMS JSON string containing clusterer parameters (default: {}) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Happy Splitter file (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Output JSON file to store the predictions (default: None) -r REPEAT_NUM, --repeat_num REPEAT_NUM Repeat number (default: 0) (default: 0)","title":"happy-scikit-unsupervised-build"},{"location":"happy/happy_tools/happy-scikit-unsupervised-build/#command-line","text":"usage: happy-scikit-unsupervised-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] [-S PIXEL_SELECTORS] [-m CLUSTERER_METHOD] [-p CLUSTERER_PARAMS] -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER [-r REPEAT_NUM] Evaluate clustering on hyperspectral data using specified clusterer and pixel selector. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Directory containing the hyperspectral data (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 snv derivative pca -n 5 -p 20) -S PIXEL_SELECTORS, --pixel_selectors PIXEL_SELECTORS The pixel selectors to use. Either pixel selector command-line(s) or file with one pixel selector command-line per line. (default: simple-ps -n 32 -b) -m CLUSTERER_METHOD, --clusterer_method CLUSTERER_METHOD Clusterer name (e.g., kmeans,agglomerative,spectral,dbscan,meanshift) or full class name (default: kmeans) -p CLUSTERER_PARAMS, --clusterer_params CLUSTERER_PARAMS JSON string containing clusterer parameters (default: {}) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Happy Splitter file (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Output JSON file to store the predictions (default: None) -r REPEAT_NUM, --repeat_num REPEAT_NUM Repeat number (default: 0) (default: 0)","title":"Command-line"},{"location":"happy/happy_tools/happy-splitter/","text":"Command-line # usage: happy-splitter [-h] -d HAPPY_BASE_FOLDER [-r NUM_REPEATS] [-f NUM_FOLDS] [-t TRAIN_PERCENT] [-v VALIDATION_PERCENT] [-R] [-H HOLDOUT_PERCENT] -o OUTPUT_FILE Generate train/validation/test splits for Happy data. optional arguments: -h, --help show this help message and exit -d HAPPY_BASE_FOLDER, --happy_base_folder HAPPY_BASE_FOLDER Path to the Happy base folder (default: None) -r NUM_REPEATS, --num_repeats NUM_REPEATS Number of repeats (default: 1) -f NUM_FOLDS, --num_folds NUM_FOLDS Number of folds (default: 1) -t TRAIN_PERCENT, --train_percent TRAIN_PERCENT Percentage of data in the training set (default: 70.0) -v VALIDATION_PERCENT, --validation_percent VALIDATION_PERCENT Percentage of data in the validation set (default: 10.0) -R, --use_regions Use regions in generating splits (default: False) -H HOLDOUT_PERCENT, --holdout_percent HOLDOUT_PERCENT Percentage of data to hold out as a holdout set (default: None) -o OUTPUT_FILE, --output_file OUTPUT_FILE Path to the output split file (default: output_split.json)","title":"happy-splitter"},{"location":"happy/happy_tools/happy-splitter/#command-line","text":"usage: happy-splitter [-h] -d HAPPY_BASE_FOLDER [-r NUM_REPEATS] [-f NUM_FOLDS] [-t TRAIN_PERCENT] [-v VALIDATION_PERCENT] [-R] [-H HOLDOUT_PERCENT] -o OUTPUT_FILE Generate train/validation/test splits for Happy data. optional arguments: -h, --help show this help message and exit -d HAPPY_BASE_FOLDER, --happy_base_folder HAPPY_BASE_FOLDER Path to the Happy base folder (default: None) -r NUM_REPEATS, --num_repeats NUM_REPEATS Number of repeats (default: 1) -f NUM_FOLDS, --num_folds NUM_FOLDS Number of folds (default: 1) -t TRAIN_PERCENT, --train_percent TRAIN_PERCENT Percentage of data in the training set (default: 70.0) -v VALIDATION_PERCENT, --validation_percent VALIDATION_PERCENT Percentage of data in the validation set (default: 10.0) -R, --use_regions Use regions in generating splits (default: False) -H HOLDOUT_PERCENT, --holdout_percent HOLDOUT_PERCENT Percentage of data to hold out as a holdout set (default: None) -o OUTPUT_FILE, --output_file OUTPUT_FILE Path to the output split file (default: output_split.json)","title":"Command-line"},{"location":"happy/happy_tools/installation/","text":"Prerequisites # Windows/WSL : Ubuntu 2022.04.x from the Microsoft store Python Virtual environments sudo apt install virtualenv python3-tk Installation # In the home directory, create a Python virtual environment in directory happy with access to the system-wide installed libraries: virtualenv --system-site-packages -p /usr/bin/python3 happy Install the happy-tools straight from the repository: ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git Updating # Once installed, you can update the library as follows: ./happy/bin/pip uninstall -y happy-tools ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git Uninstall # You can completely remove the tools by simply deleting the happy directory: rm -Rf ./happy","title":"Installation"},{"location":"happy/happy_tools/installation/#prerequisites","text":"Windows/WSL : Ubuntu 2022.04.x from the Microsoft store Python Virtual environments sudo apt install virtualenv python3-tk","title":"Prerequisites"},{"location":"happy/happy_tools/installation/#installation","text":"In the home directory, create a Python virtual environment in directory happy with access to the system-wide installed libraries: virtualenv --system-site-packages -p /usr/bin/python3 happy Install the happy-tools straight from the repository: ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git","title":"Installation"},{"location":"happy/happy_tools/installation/#updating","text":"Once installed, you can update the library as follows: ./happy/bin/pip uninstall -y happy-tools ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git","title":"Updating"},{"location":"happy/happy_tools/installation/#uninstall","text":"You can completely remove the tools by simply deleting the happy directory: rm -Rf ./happy","title":"Uninstall"},{"location":"happy/happy_tools_keras/","text":"Additional tools utilizing the Keras deep learning library, available through happy-tools-keras : happy-keras-pixel-regression-build - evaluate a Keras-based pixel regression model happy-keras-segmentation-build - builds a Keras-based pixel segmentation model happy-keras-unsupervised-build - builds a Keras-based pixel segmentation model These tools are available from the Python virtual environment that they were installed. E.g., when following the installation instructions on this website, the tools would be located in the following directory in the user's home folder: happy/bin","title":"Overview"},{"location":"happy/happy_tools_keras/happy-keras-pixel-regression-build/","text":"Command-line # usage: happy-keras-pixel-regression-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] -t TARGET -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER Evaluate a Keras-based pixel regression model. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Path to the data folder (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: crop -W 320 -H 648 wavelength-subset -f 60 -t 189 sni snv derivative -w 15 -d 1 pad -W 320 -H 648 -v 0 down- sample) -t TARGET, --target TARGET Name of the target variable (default: None) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Path to JSON file containing splits (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Path to the output folder (default: None)","title":"happy-keras-pixel-regression-build"},{"location":"happy/happy_tools_keras/happy-keras-pixel-regression-build/#command-line","text":"usage: happy-keras-pixel-regression-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] -t TARGET -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER Evaluate a Keras-based pixel regression model. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Path to the data folder (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: crop -W 320 -H 648 wavelength-subset -f 60 -t 189 sni snv derivative -w 15 -d 1 pad -W 320 -H 648 -v 0 down- sample) -t TARGET, --target TARGET Name of the target variable (default: None) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Path to JSON file containing splits (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Path to the output folder (default: None)","title":"Command-line"},{"location":"happy/happy_tools_keras/happy-keras-segmentation-build/","text":"Command-line # usage: happy-keras-segmentation-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] -t TARGET [-n NUM_CLASSES] -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER Build a Keras-based pixel segmentation model. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Path to the data folder (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 sni snv derivative -w 15 -d 1 pad -W 128 -H 128 -v 0) -t TARGET, --target TARGET Name of the target variable (default: None) -n NUM_CLASSES, --num_classes NUM_CLASSES The number of classes, used for generating the mapping (default: 4) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Path to JSON file containing splits (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Path to the output folder (default: None)","title":"happy-keras-segmentation-build"},{"location":"happy/happy_tools_keras/happy-keras-segmentation-build/#command-line","text":"usage: happy-keras-segmentation-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] -t TARGET [-n NUM_CLASSES] -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER Build a Keras-based pixel segmentation model. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Path to the data folder (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 sni snv derivative -w 15 -d 1 pad -W 128 -H 128 -v 0) -t TARGET, --target TARGET Name of the target variable (default: None) -n NUM_CLASSES, --num_classes NUM_CLASSES The number of classes, used for generating the mapping (default: 4) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Path to JSON file containing splits (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Path to the output folder (default: None)","title":"Command-line"},{"location":"happy/happy_tools_keras/happy-keras-unsupervised-build/","text":"Command-line # usage: happy-keras-unsupervised-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] -t TARGET [-n NUM_CLUSTERS] -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER Build a Keras-based unsupervised segmentation model. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Path to the data folder (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 snv derivative pad -W 128 -H 128 -v 0) -t TARGET, --target TARGET Name of the target variable (default: None) -n NUM_CLUSTERS, --num_clusters NUM_CLUSTERS The number of clusters to use (default: 4) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Path to JSON file containing splits (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Path to the output folder (default: None)","title":"happy-keras-unsupervised-build"},{"location":"happy/happy_tools_keras/happy-keras-unsupervised-build/#command-line","text":"usage: happy-keras-unsupervised-build [-h] -d DATA_FOLDER [-P PREPROCESSORS] -t TARGET [-n NUM_CLUSTERS] -s HAPPY_SPLITTER_FILE -o OUTPUT_FOLDER Build a Keras-based unsupervised segmentation model. optional arguments: -h, --help show this help message and exit -d DATA_FOLDER, --data_folder DATA_FOLDER Path to the data folder (default: None) -P PREPROCESSORS, --preprocessors PREPROCESSORS The preprocessors to apply to the data. Either preprocessor command-line(s) or file with one preprocessor command-line per line. (default: wavelength-subset -f 60 -t 189 snv derivative pad -W 128 -H 128 -v 0) -t TARGET, --target TARGET Name of the target variable (default: None) -n NUM_CLUSTERS, --num_clusters NUM_CLUSTERS The number of clusters to use (default: 4) -s HAPPY_SPLITTER_FILE, --happy_splitter_file HAPPY_SPLITTER_FILE Path to JSON file containing splits (default: None) -o OUTPUT_FOLDER, --output_folder OUTPUT_FOLDER Path to the output folder (default: None)","title":"Command-line"},{"location":"happy/happy_tools_keras/installation/","text":"Prerequisites # Windows/WSL : Ubuntu 2022.04.x from the Microsoft store Python Virtual environments sudo apt install virtualenv python3-tk Installation # In the home directory, create a Python virtual environment in directory happy with access to the system-wide installed libraries: virtualenv --system-site-packages -p /usr/bin/python3 happy Install the happy-tools straight from the repository: ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git ./happy/bin/pip install git+https://github.com/wairas/happy-tools-keras.git Updating # Once installed, you can update the library as follows: ./happy/bin/pip uninstall -y happy-tools happy-tools-keras ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git ./happy/bin/pip install git+https://github.com/wairas/happy-tools-keras.git Uninstall # You can completely remove the tools by simply deleting the happy directory: rm -Rf ./happy","title":"Installation"},{"location":"happy/happy_tools_keras/installation/#prerequisites","text":"Windows/WSL : Ubuntu 2022.04.x from the Microsoft store Python Virtual environments sudo apt install virtualenv python3-tk","title":"Prerequisites"},{"location":"happy/happy_tools_keras/installation/#installation","text":"In the home directory, create a Python virtual environment in directory happy with access to the system-wide installed libraries: virtualenv --system-site-packages -p /usr/bin/python3 happy Install the happy-tools straight from the repository: ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git ./happy/bin/pip install git+https://github.com/wairas/happy-tools-keras.git","title":"Installation"},{"location":"happy/happy_tools_keras/installation/#updating","text":"Once installed, you can update the library as follows: ./happy/bin/pip uninstall -y happy-tools happy-tools-keras ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git ./happy/bin/pip install git+https://github.com/wairas/happy-tools-keras.git","title":"Updating"},{"location":"happy/happy_tools_keras/installation/#uninstall","text":"You can completely remove the tools by simply deleting the happy directory: rm -Rf ./happy","title":"Uninstall"}]}