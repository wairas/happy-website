{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Hyper-spectral Imaging Group at the University of Waikato.","title":"Home"},{"location":"publications/","text":"2023 # Abeysekera SK, Robinson A, Ooi MPL, Kuang YC, Manley-Harris M, Holmes W, Hirst E, Nowak J, Caddie M, Steinhorn G, Demidenko S. Sparse reproducible machine learning for near infrared hyperspectral imaging: Estimating the tetrahydrocannabinolic acid concentration in Cannabis sativa L. 1 Feb 2023. Industrial Crops and Products, 10.1016/j.indcrop.2022.116137","title":"Publications"},{"location":"publications/#2023","text":"Abeysekera SK, Robinson A, Ooi MPL, Kuang YC, Manley-Harris M, Holmes W, Hirst E, Nowak J, Caddie M, Steinhorn G, Demidenko S. Sparse reproducible machine learning for near infrared hyperspectral imaging: Estimating the tetrahydrocannabinolic acid concentration in Cannabis sativa L. 1 Feb 2023. Industrial Crops and Products, 10.1016/j.indcrop.2022.116137","title":"2023"},{"location":"team/","text":"The Hyper-spectral imaging team: Associate Professor Melanie Ooi Senior Lecturer Ye Chow Kuang Professor Geoffrey Holmes Dr Sanush Abeysekera Mr Dale Fletcher Mr Peter Reutemann","title":"Team"},{"location":"happy/","text":"Applications, Python libraries and Docker images for hyper-spectral data processing and modelling. Installation # The tools have been tested on Linux (Ubuntu/Debian) and under Windows with WSL2 using Ubuntu 22.04.x. Notes on WSL2 Installing the tools Usage # Once installed using the above instructions, you can launch graphical tools and Docker images (start/stop) using the happy-launch.sh script from your home directory: ./happy-launch.sh Tools # More details on the tools that are available through the HAPPy project can be found on the respective tool pages: ADAMS happy-tools Segment Anything Segment Anything in High Quality NB: These pages also contain detailed instructions on how to install them, which you can ignore if you used the happy-setup.sh installation script.","title":"Introduction"},{"location":"happy/#installation","text":"The tools have been tested on Linux (Ubuntu/Debian) and under Windows with WSL2 using Ubuntu 22.04.x. Notes on WSL2 Installing the tools","title":"Installation"},{"location":"happy/#usage","text":"Once installed using the above instructions, you can launch graphical tools and Docker images (start/stop) using the happy-launch.sh script from your home directory: ./happy-launch.sh","title":"Usage"},{"location":"happy/#tools","text":"More details on the tools that are available through the HAPPy project can be found on the respective tool pages: ADAMS happy-tools Segment Anything Segment Anything in High Quality NB: These pages also contain detailed instructions on how to install them, which you can ignore if you used the happy-setup.sh installation script.","title":"Tools"},{"location":"happy/adams/","text":"ADAMS-based framework that can be used for annotating images using its powerful workflow engine. Requirements # OpenJDK 11+ Windows: adoptium.net/temurin Debian/Ubuntu: sudo apt install openjdk-11-jdk Installation # Download a snapshot (in ZIP format) adams.cms.waikato.ac.nz/snapshots/happy/ Unzip the ZIP archive and rename the generated directory to happy-adams Starting the application # Start the user interface with: Windows: happy-adams\\bin\\start_gui.bat Linux: happy-adams/bin/start_gui.sh Available flows # Of the flows that are come with the Happy ADAMS framework, the following ones are relevant to the Happy project: adams-imaging-annotate_objects.flow - generating annotations for object detection (bounding box or polygon) adams-imaging-image_segmentation_annotation.flow - generating annotations for image segmentation (pixel-level classification) adams-imaging-ext_run-sam.flow - downloads and runs SAM (Segment Anything Model) via Docker (can be used within the above two flows as a separate annotation tool) Tutorials # Instructions on how to use these flows are available from the Applied Deep Learning website, specifically: object detection image segmentation Preview browser # The Preview browser (from the Visualization menu) can be used for viewing PNG/OPEX JSON files that were export from the happy-envi-viewer tool. Using the ObjectLocationsFromReport with the OpexObjectLocationsReader you can generate an overlay of the annotations like this: The options used can be seen here: And here as a configuration setup that you can paste via the drop-down button in the top-right corner of the options dialog: # Project: adams # Date: 2023-08-23 16:26:05 # User: fracpete # Charset: UTF-8 # Modules: adams-core,adams-docker,adams-imaging,adams-imaging-ext,adams-json,adams-meta,adams-net,adams-redis,adams-spreadsheet,adams-xml # adams.gui.tools.previewbrowser.ObjectLocationsFromReport -image-reader adams.data.io.input.JAIImageReader -reader adams.data.io.input.OpexObjectLocationsReader -type-color-provider adams.gui.visualization.core.DefaultColorProvider -label-anchor MIDDLE_CENTER -shape-color-provider adams.gui.visualization.core.TranslucentColorProvider -provider adams.gui.visualization.core.DefaultColorProvider -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal adams.data.overlappingobjectremoval.PassThrough -show-object-panel true","title":"ADAMS"},{"location":"happy/adams/#requirements","text":"OpenJDK 11+ Windows: adoptium.net/temurin Debian/Ubuntu: sudo apt install openjdk-11-jdk","title":"Requirements"},{"location":"happy/adams/#installation","text":"Download a snapshot (in ZIP format) adams.cms.waikato.ac.nz/snapshots/happy/ Unzip the ZIP archive and rename the generated directory to happy-adams","title":"Installation"},{"location":"happy/adams/#starting-the-application","text":"Start the user interface with: Windows: happy-adams\\bin\\start_gui.bat Linux: happy-adams/bin/start_gui.sh","title":"Starting the application"},{"location":"happy/adams/#available-flows","text":"Of the flows that are come with the Happy ADAMS framework, the following ones are relevant to the Happy project: adams-imaging-annotate_objects.flow - generating annotations for object detection (bounding box or polygon) adams-imaging-image_segmentation_annotation.flow - generating annotations for image segmentation (pixel-level classification) adams-imaging-ext_run-sam.flow - downloads and runs SAM (Segment Anything Model) via Docker (can be used within the above two flows as a separate annotation tool)","title":"Available flows"},{"location":"happy/adams/#tutorials","text":"Instructions on how to use these flows are available from the Applied Deep Learning website, specifically: object detection image segmentation","title":"Tutorials"},{"location":"happy/adams/#preview-browser","text":"The Preview browser (from the Visualization menu) can be used for viewing PNG/OPEX JSON files that were export from the happy-envi-viewer tool. Using the ObjectLocationsFromReport with the OpexObjectLocationsReader you can generate an overlay of the annotations like this: The options used can be seen here: And here as a configuration setup that you can paste via the drop-down button in the top-right corner of the options dialog: # Project: adams # Date: 2023-08-23 16:26:05 # User: fracpete # Charset: UTF-8 # Modules: adams-core,adams-docker,adams-imaging,adams-imaging-ext,adams-json,adams-meta,adams-net,adams-redis,adams-spreadsheet,adams-xml # adams.gui.tools.previewbrowser.ObjectLocationsFromReport -image-reader adams.data.io.input.JAIImageReader -reader adams.data.io.input.OpexObjectLocationsReader -type-color-provider adams.gui.visualization.core.DefaultColorProvider -label-anchor MIDDLE_CENTER -shape-color-provider adams.gui.visualization.core.TranslucentColorProvider -provider adams.gui.visualization.core.DefaultColorProvider -finder adams.data.objectfinder.AllFinder -overlap-detection adams.data.objectoverlap.AreaRatio -overlap-removal adams.data.overlappingobjectremoval.PassThrough -show-object-panel true","title":"Preview browser"},{"location":"happy/installation/","text":"Prerequisites # Linux (Ubuntu/Debian) Docker ( sudo apt install docker.io ) redis-server ( sudo apt install redis-server ) wget ( sudo apt install wget ) Windows/WSL2 Ubuntu 22.04.x from the Microsoft store Docker redis-server ( sudo apt install redis-server ) wget ( sudo apt install wget ) Installation # Go to your home directory: cd ~ Download the happy-setup.sh script and make it executable: wget -O happy-setup.sh https://raw.githubusercontent.com/wairas/happy-scripts/main/happy-setup.sh chmod a+x happy-setup.sh Execute the script: ./happy-setup.sh Minimal installation items to execute: Prepare system (ensures that all required system libraries are present) Install Happy Tools Install SAM-HQ (or if you prefer, Install SAM ) Notes: SAM and SAM-HQ can be installed in parallel, but only one of them can actively running, as they both use the same redis channels for communication (that way they are interchangeable). ADAMS can be used for annotating objects in your scanned images.","title":"Installation"},{"location":"happy/installation/#prerequisites","text":"Linux (Ubuntu/Debian) Docker ( sudo apt install docker.io ) redis-server ( sudo apt install redis-server ) wget ( sudo apt install wget ) Windows/WSL2 Ubuntu 22.04.x from the Microsoft store Docker redis-server ( sudo apt install redis-server ) wget ( sudo apt install wget )","title":"Prerequisites"},{"location":"happy/installation/#installation","text":"Go to your home directory: cd ~ Download the happy-setup.sh script and make it executable: wget -O happy-setup.sh https://raw.githubusercontent.com/wairas/happy-scripts/main/happy-setup.sh chmod a+x happy-setup.sh Execute the script: ./happy-setup.sh Minimal installation items to execute: Prepare system (ensures that all required system libraries are present) Install Happy Tools Install SAM-HQ (or if you prefer, Install SAM ) Notes: SAM and SAM-HQ can be installed in parallel, but only one of them can actively running, as they both use the same redis channels for communication (that way they are interchangeable). ADAMS can be used for annotating objects in your scanned images.","title":"Installation"},{"location":"happy/sam-hq/","text":"Segment Anything in High Quality (SAM-HQ) works like SAM and consists of pretrained models that perform image segmentation on RGB images and can aid the human in the annotation process. Prerequisites # Linux # docker redis-server ( sudo apt install redis-server ) Windows # WSL2 using 22.04.x docker ( instructions ) redis-server ( sudo apt install redis-server ) Directories # sam-hq | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM-HQ models You can create the structure using the following command: mkdir -p sam-hq/cache \\ mkdir -p sam-hq/models Pretrained models # Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam-hq/models directory, run the following command: wget https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_l.pth Service scripts (WSL2 without Docker Desktop UI) # Create a bash script happy_samhq_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_samhq_start.sh Create a bash script happy_samhq_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happyhq_sam_stop.sh SAM scripts # In the sam-hq directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam-hq:2023-08-17_cuda11.6 \\ samhq_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_hq_vit_l.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . NB: This script uses the sam_in and sam_out Redis channels to make it a drop-in replacement for SAM in the happy-envi-viewer . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]amhq_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh . Starting # Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_samhq_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background. SAM # In the sam-hq directory, execute the start.sh script. Stopping # SAM # In the sam-hq directory, execute the stop.sh script. Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_samhq_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"SAM-HQ"},{"location":"happy/sam-hq/#prerequisites","text":"","title":"Prerequisites"},{"location":"happy/sam-hq/#linux","text":"docker redis-server ( sudo apt install redis-server )","title":"Linux"},{"location":"happy/sam-hq/#windows","text":"WSL2 using 22.04.x docker ( instructions ) redis-server ( sudo apt install redis-server )","title":"Windows"},{"location":"happy/sam-hq/#directories","text":"sam-hq | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM-HQ models You can create the structure using the following command: mkdir -p sam-hq/cache \\ mkdir -p sam-hq/models","title":"Directories"},{"location":"happy/sam-hq/#pretrained-models","text":"Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam-hq/models directory, run the following command: wget https://huggingface.co/lkeab/hq-sam/resolve/main/sam_hq_vit_l.pth","title":"Pretrained models"},{"location":"happy/sam-hq/#service-scripts-wsl2-without-docker-desktop-ui","text":"Create a bash script happy_samhq_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_samhq_start.sh Create a bash script happy_samhq_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happyhq_sam_stop.sh","title":"Service scripts (WSL2 without Docker Desktop UI)"},{"location":"happy/sam-hq/#sam-scripts","text":"In the sam-hq directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam-hq:2023-08-17_cuda11.6 \\ samhq_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_hq_vit_l.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . NB: This script uses the sam_in and sam_out Redis channels to make it a drop-in replacement for SAM in the happy-envi-viewer . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]amhq_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh .","title":"SAM scripts"},{"location":"happy/sam-hq/#starting","text":"","title":"Starting"},{"location":"happy/sam-hq/#docker-and-redis-wsl2-without-docker-desktop-ui","text":"sudo /usr/local/bin/happy_samhq_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/sam-hq/#sam","text":"In the sam-hq directory, execute the start.sh script.","title":"SAM"},{"location":"happy/sam-hq/#stopping","text":"","title":"Stopping"},{"location":"happy/sam-hq/#sam_1","text":"In the sam-hq directory, execute the stop.sh script.","title":"SAM"},{"location":"happy/sam-hq/#docker-and-redis-wsl2-without-docker-desktop-ui_1","text":"sudo /usr/local/bin/happy_samhq_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/","text":"Facebook's Segment Anything (SAM) are pretrained models that perform image segmentation on RGB images and can aid the human in the annotation process. Prerequisites # Linux # docker redis-server ( sudo apt install redis-server ) Windows # WSL2 using 22.04.x docker ( instructions ) redis-server ( sudo apt install redis-server ) Directories # sam | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM models You can create the structure using the following command: mkdir -p sam/cache \\ mkdir -p sam/models Pretrained models # Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam/models directory, run the following command: wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth Service scripts (WSL2 without Docker Desktop UI) # Create a bash script happy_sam_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_sam_start.sh Create a bash script happy_sam_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happy_sam_stop.sh SAM scripts # In the sam directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam:2023-04-16_cuda11.6 \\ sam_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_vit_l_0b3195.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]am_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh . Starting # Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_sam_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background. SAM # In the sam directory, execute the start.sh script. Stopping # SAM # In the sam directory, execute the stop.sh script. Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_sam_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"SAM"},{"location":"happy/sam/#prerequisites","text":"","title":"Prerequisites"},{"location":"happy/sam/#linux","text":"docker redis-server ( sudo apt install redis-server )","title":"Linux"},{"location":"happy/sam/#windows","text":"WSL2 using 22.04.x docker ( instructions ) redis-server ( sudo apt install redis-server )","title":"Windows"},{"location":"happy/sam/#directories","text":"sam | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM models You can create the structure using the following command: mkdir -p sam/cache \\ mkdir -p sam/models","title":"Directories"},{"location":"happy/sam/#pretrained-models","text":"Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam/models directory, run the following command: wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth","title":"Pretrained models"},{"location":"happy/sam/#service-scripts-wsl2-without-docker-desktop-ui","text":"Create a bash script happy_sam_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_sam_start.sh Create a bash script happy_sam_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happy_sam_stop.sh","title":"Service scripts (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/#sam-scripts","text":"In the sam directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam:2023-04-16_cuda11.6 \\ sam_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_vit_l_0b3195.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]am_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh .","title":"SAM scripts"},{"location":"happy/sam/#starting","text":"","title":"Starting"},{"location":"happy/sam/#docker-and-redis-wsl2-without-docker-desktop-ui","text":"sudo /usr/local/bin/happy_sam_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/#sam","text":"In the sam directory, execute the start.sh script.","title":"SAM"},{"location":"happy/sam/#stopping","text":"","title":"Stopping"},{"location":"happy/sam/#sam_1","text":"In the sam directory, execute the stop.sh script.","title":"SAM"},{"location":"happy/sam/#docker-and-redis-wsl2-without-docker-desktop-ui_1","text":"sudo /usr/local/bin/happy_sam_stop.sh NB: This will also stop any running SAM/SAM-HQ process.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/wsl2/","text":"Though all the tools are working under the Windows Subsystem for Linux , Docker and graphical applications only work as long as you are using version 2 of WSL and your Windows 10 is at least build 19044 (no restrictions with Windows 11) . Here is how to switch WSL to version 2 as default by running the following command: wsl --set-default-version 2 You can view all your current images and what WSL version they are using with the following command: wsl --list -v If you want to switch an existing image (e.g., Ubuntu-22.04 ) to version 2, then you can run the following command: wsl --set-version Ubuntu-22.04 2 Source: https://stackoverflow.com/a/73164601","title":"WSL2"},{"location":"happy/happy_tools/","text":"happy-tools contains several command-line utilities and graphical viewers for HSI files: happy-data-viewer - for viewing HAPPy data folders happy-envi-viewer - for viewing ENVI HSI images happy-generate-image-regions-objects - generates datasets as numpy cubes for deep learning happy-hdr-info - outputs information on ENVI HDR files happy-hsi2csv - converts HSI images into CSV happy-hsi2rbg - generates fake RGB PNG files from HSI images happy-mat-info - outputs Matlab struct information happy-opex2happy - converts OPEX JSON annotations and PNG images into happy data structures happy-plot-preproc - plots set of pixels using various preprocessors happy-scikit-regression-build - evaluates regression models on HAPPy data happy-scikit-unsupervised-build - evaluates cluster models on HAPPy data happy-splitter - for generating train/validation/test splits for HAPPy data These tools are available from the Python virtual environment that they were installed. E.g., when following the installation instructions on this website, the tools would be located in the following directory in the user's home folder: happy/bin","title":"Overview"},{"location":"happy/happy_tools/happy-data-viewer/","text":"Used for viewing data that has been converted into a HAPPy folder structure. Command-line # usage: happy-data-viewer [-h] base_folder Viewer for Happy data folder structures. positional arguments: base_folder Base folder for HappyReader optional arguments: -h, --help show this help message and exit","title":"happy-data-viewer"},{"location":"happy/happy_tools/happy-data-viewer/#command-line","text":"usage: happy-data-viewer [-h] base_folder Viewer for Happy data folder structures. positional arguments: base_folder Base folder for HappyReader optional arguments: -h, --help show this help message and exit","title":"Command-line"},{"location":"happy/happy_tools/happy-envi-viewer/","text":"Usage # Image tab # Via the File menu, you can load an ENVI file representing a sample scan. From that menu, you can also select black and white reference ENVI files that get automatically applied to the scan: At the bottom of the window, you get a quick info on what dimensions the scan has (width, height and channels). The three sliders allow you to select the channels from the hyper-spectral image to act as red, green and blue channel for the fake RGB image that is being displayed. Left-clicking on the label next to the slider, depicting the current channel value, pops up a dialog for entering a specific channel. If default bands are defined in the ENVI header and auto-detect channels is enabled on the Options tab , then these will get used when loading the file. Info tab # On the Info tab, you can see what files are currently loaded and what dimensions these files have: Options tab # On the Options tab, you can change various view settings, how to connect to SAM and how annotations appear: Annotations # The envi-viewer also allows you to annotate images and then export them. The image will be exported as PNG using the currently selected channels. Any annotations present will get exported as JSON, using the OPEX format . Such annotations in OPEX format can be viewed in the ADAMS Preview browser . General mouse usage: Left-clicking on the image sets a marker point. Left-clicking while holding the CTRL key removes any marker points. Left-clicking on an existing annotation shape while holding the SHIFT key allows you to enter a label for that shape (e.g., white_ref or leaf ). Tools menu: Clear annotations - removes any annotations Clear markers - removes all marker points Remove last annotation - removes the most recent annotation that was added (can be repeated till there are no more annotations) Polygon - turns current marker points into polygon SAM - uses current marker points as prompt points for SAM Polygons # The simplest way of annotating that does not require any further tools is by using polygons. First define the outline of the object with marker points: Once at least three marker points have been put on the image, selecting Polygon from the Tools menu turns them into a polygon annotation: SAM # Using SAM , you can easily annotate complex shapes accurately. Though SAM can run on a CPU, it is recommended to use a computer with a NVIDIA GPU as it will speed up the detection process by at least 10 times. SAM requires you to at least provide a single marker on the object that you want to trace the shape for. Depending on the object and how well it is separated from the background, how much the colors on the object change, you may have to provide more than one marker point to better guide the detection: The result looks then like this: Command-line # Using the command-line options, you can preset the options in the user interface and also load scan, black and white reference files: usage: happy-envi-viewer [-h] [-s SCAN] [-f BLACK_REFERENCE] [-w WHITE_REFERENCE] [-r INT] [-g INT] [-b INT] [--autodetect_channels] [--keep_aspectratio] [--check_scan_dimensions] [--annotation_color HEXCOLOR] [--redis_host HOST] [--redis_port PORT] [--redis_pw PASSWORD] [--redis_in CHANNEL] [--redis_out CHANNEL] [--redis_connect] [--marker_size INT] [--marker_color HEXCOLOR] [--min_obj_size INT] ENVI Hyper-spectral Image Viewer. Offers contour detection using SAM (Segment- Anything: https://github.com/waikato-datamining/pytorch/tree/master/segment- anything) optional arguments: -h, --help show this help message and exit -s SCAN, --scan SCAN Path to the scan file (ENVI format) (default: None) -f BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -r INT, --scale_r INT the wave length to use for the red channel (default: None) -g INT, --scale_g INT the wave length to use for the green channel (default: None) -b INT, --scale_b INT the wave length to use for the blue channel (default: None) --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: None) --keep_aspectratio whether to keep the aspect ratio (default: None) --check_scan_dimensions whether to compare the dimensions of subsequently loaded scans and output a warning if they differ (default: None) --annotation_color HEXCOLOR the color to use for the annotations like contours (hex color) (default: None) --redis_host HOST The Redis host to connect to (IP or hostname) (default: None) --redis_port PORT The port the Redis server is listening on (default: None) --redis_pw PASSWORD The password to use with the Redis server (default: None) --redis_in CHANNEL The channel that SAM is receiving images on (default: None) --redis_out CHANNEL The channel that SAM is broadcasting the detections on (default: None) --redis_connect whether to immediately connect to the Redis host (default: None) --marker_size INT The size in pixels for the SAM points (default: None) --marker_color HEXCOLOR the color to use for the SAM points (hex color) (default: None) --min_obj_size INT The minimum size that SAM contours need to have (<= 0 for no minimum) (default: None)","title":"happy-envi-viewer"},{"location":"happy/happy_tools/happy-envi-viewer/#usage","text":"","title":"Usage"},{"location":"happy/happy_tools/happy-envi-viewer/#image-tab","text":"Via the File menu, you can load an ENVI file representing a sample scan. From that menu, you can also select black and white reference ENVI files that get automatically applied to the scan: At the bottom of the window, you get a quick info on what dimensions the scan has (width, height and channels). The three sliders allow you to select the channels from the hyper-spectral image to act as red, green and blue channel for the fake RGB image that is being displayed. Left-clicking on the label next to the slider, depicting the current channel value, pops up a dialog for entering a specific channel. If default bands are defined in the ENVI header and auto-detect channels is enabled on the Options tab , then these will get used when loading the file.","title":"Image tab"},{"location":"happy/happy_tools/happy-envi-viewer/#info-tab","text":"On the Info tab, you can see what files are currently loaded and what dimensions these files have:","title":"Info tab"},{"location":"happy/happy_tools/happy-envi-viewer/#options-tab","text":"On the Options tab, you can change various view settings, how to connect to SAM and how annotations appear:","title":"Options tab"},{"location":"happy/happy_tools/happy-envi-viewer/#annotations","text":"The envi-viewer also allows you to annotate images and then export them. The image will be exported as PNG using the currently selected channels. Any annotations present will get exported as JSON, using the OPEX format . Such annotations in OPEX format can be viewed in the ADAMS Preview browser . General mouse usage: Left-clicking on the image sets a marker point. Left-clicking while holding the CTRL key removes any marker points. Left-clicking on an existing annotation shape while holding the SHIFT key allows you to enter a label for that shape (e.g., white_ref or leaf ). Tools menu: Clear annotations - removes any annotations Clear markers - removes all marker points Remove last annotation - removes the most recent annotation that was added (can be repeated till there are no more annotations) Polygon - turns current marker points into polygon SAM - uses current marker points as prompt points for SAM","title":"Annotations"},{"location":"happy/happy_tools/happy-envi-viewer/#polygons","text":"The simplest way of annotating that does not require any further tools is by using polygons. First define the outline of the object with marker points: Once at least three marker points have been put on the image, selecting Polygon from the Tools menu turns them into a polygon annotation:","title":"Polygons"},{"location":"happy/happy_tools/happy-envi-viewer/#sam","text":"Using SAM , you can easily annotate complex shapes accurately. Though SAM can run on a CPU, it is recommended to use a computer with a NVIDIA GPU as it will speed up the detection process by at least 10 times. SAM requires you to at least provide a single marker on the object that you want to trace the shape for. Depending on the object and how well it is separated from the background, how much the colors on the object change, you may have to provide more than one marker point to better guide the detection: The result looks then like this:","title":"SAM"},{"location":"happy/happy_tools/happy-envi-viewer/#command-line","text":"Using the command-line options, you can preset the options in the user interface and also load scan, black and white reference files: usage: happy-envi-viewer [-h] [-s SCAN] [-f BLACK_REFERENCE] [-w WHITE_REFERENCE] [-r INT] [-g INT] [-b INT] [--autodetect_channels] [--keep_aspectratio] [--check_scan_dimensions] [--annotation_color HEXCOLOR] [--redis_host HOST] [--redis_port PORT] [--redis_pw PASSWORD] [--redis_in CHANNEL] [--redis_out CHANNEL] [--redis_connect] [--marker_size INT] [--marker_color HEXCOLOR] [--min_obj_size INT] ENVI Hyper-spectral Image Viewer. Offers contour detection using SAM (Segment- Anything: https://github.com/waikato-datamining/pytorch/tree/master/segment- anything) optional arguments: -h, --help show this help message and exit -s SCAN, --scan SCAN Path to the scan file (ENVI format) (default: None) -f BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -r INT, --scale_r INT the wave length to use for the red channel (default: None) -g INT, --scale_g INT the wave length to use for the green channel (default: None) -b INT, --scale_b INT the wave length to use for the blue channel (default: None) --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: None) --keep_aspectratio whether to keep the aspect ratio (default: None) --check_scan_dimensions whether to compare the dimensions of subsequently loaded scans and output a warning if they differ (default: None) --annotation_color HEXCOLOR the color to use for the annotations like contours (hex color) (default: None) --redis_host HOST The Redis host to connect to (IP or hostname) (default: None) --redis_port PORT The port the Redis server is listening on (default: None) --redis_pw PASSWORD The password to use with the Redis server (default: None) --redis_in CHANNEL The channel that SAM is receiving images on (default: None) --redis_out CHANNEL The channel that SAM is broadcasting the detections on (default: None) --redis_connect whether to immediately connect to the Redis host (default: None) --marker_size INT The size in pixels for the SAM points (default: None) --marker_color HEXCOLOR the color to use for the SAM points (hex color) (default: None) --min_obj_size INT The minimum size that SAM contours need to have (<= 0 for no minimum) (default: None)","title":"Command-line"},{"location":"happy/happy_tools/happy-generate-image-regions-objects/","text":"Command-line # usage: happy-generate-image-regions-objects [-h] source_folder output_folder Generate datasets as numpy cubes, to be loaded into deep learning datasets. positional arguments: source_folder Path to source folder containing HDR files output_folder Path to output folder optional arguments: -h, --help show this help message and exit","title":"Command-line"},{"location":"happy/happy_tools/happy-generate-image-regions-objects/#command-line","text":"usage: happy-generate-image-regions-objects [-h] source_folder output_folder Generate datasets as numpy cubes, to be loaded into deep learning datasets. positional arguments: source_folder Path to source folder containing HDR files output_folder Path to output folder optional arguments: -h, --help show this help message and exit","title":"Command-line"},{"location":"happy/happy_tools/happy-hdr-info/","text":"Command-line # usage: happy-hdr-info [-h] hdrfile Load and print information about an HDR file. positional arguments: hdrfile Path to the HDR file optional arguments: -h, --help show this help message and exit","title":"happy-hdr-info"},{"location":"happy/happy_tools/happy-hdr-info/#command-line","text":"usage: happy-hdr-info [-h] hdrfile Load and print information about an HDR file. positional arguments: hdrfile Path to the HDR file optional arguments: -h, --help show this help message and exit","title":"Command-line"},{"location":"happy/happy_tools/happy-hsi2csv/","text":"Command-line # usage: happy-hsi2csv [-h] -d DIR -m DIR -s FILE -o DIR [-M [METADATA_VALUES [METADATA_VALUES ...]]] -T TARGETS [TARGETS ...] Generates CSV files from hyper-spectral images. optional arguments: -h, --help show this help message and exit -d DIR, --data_dir DIR the directory with the hyper-spectral data files (default: None) -m DIR, --metadata_dir DIR the directory with the meta-data JSON files (default: None) -s FILE, --sample_ids FILE the JSON file with the array of sample IDs to process (default: None) -o DIR, --output_dir DIR the directory to store the results in (default: None) -M [METADATA_VALUES [METADATA_VALUES ...]], --metadata_values [METADATA_VALUES [METADATA_VALUES ...]] the meta-data values to add to the output (default: None) -T TARGETS [TARGETS ...], --targets TARGETS [TARGETS ...] the target values to generate data for (default: None)","title":"happy-hsi2csv"},{"location":"happy/happy_tools/happy-hsi2csv/#command-line","text":"usage: happy-hsi2csv [-h] -d DIR -m DIR -s FILE -o DIR [-M [METADATA_VALUES [METADATA_VALUES ...]]] -T TARGETS [TARGETS ...] Generates CSV files from hyper-spectral images. optional arguments: -h, --help show this help message and exit -d DIR, --data_dir DIR the directory with the hyper-spectral data files (default: None) -m DIR, --metadata_dir DIR the directory with the meta-data JSON files (default: None) -s FILE, --sample_ids FILE the JSON file with the array of sample IDs to process (default: None) -o DIR, --output_dir DIR the directory to store the results in (default: None) -M [METADATA_VALUES [METADATA_VALUES ...]], --metadata_values [METADATA_VALUES [METADATA_VALUES ...]] the meta-data values to add to the output (default: None) -T TARGETS [TARGETS ...], --targets TARGETS [TARGETS ...] the target values to generate data for (default: None)","title":"Command-line"},{"location":"happy/happy_tools/happy-hsi2rbg/","text":"Command-line # usage: happy-hsi2rgb [-h] -i INPUT_DIR [INPUT_DIR ...] [-r] [-e EXTENSION] [-b BLACK_REFERENCE] [-w WHITE_REFERENCE] [-a] [--red INT] [--green INT] [--blue INT] [-o OUTPUT_DIR] [--width INT] [--height INT] [-n] [-v] Fake RGB image generator for HSI files. optional arguments: -h, --help show this help message and exit -i INPUT_DIR [INPUT_DIR ...], --input_dir INPUT_DIR [INPUT_DIR ...] Path to the scan file (ENVI format) (default: None) -r, --recursive whether to traverse the directories recursively (default: False) -e EXTENSION, --extension EXTENSION The file extension to look for (default: .hdr) -b BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -a, --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: False) --red INT the wave length to use for the red channel (0-based) (default: 0) --green INT the wave length to use for the green channel (0-based) (default: 0) --blue INT the wave length to use for the blue channel (0-based) (default: 0) -o OUTPUT_DIR, --output_dir OUTPUT_DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) --width INT the width to scale the images to (<= 0 uses image dimension) (default: 0) --height INT the height to scale the images to (<= 0 uses image dimension) (default: 0) -n, --dry_run whether to omit saving the PNG images (default: False) -v, --verbose whether to be more verbose with the output (default: False)","title":"happy-hsi2rbg"},{"location":"happy/happy_tools/happy-hsi2rbg/#command-line","text":"usage: happy-hsi2rgb [-h] -i INPUT_DIR [INPUT_DIR ...] [-r] [-e EXTENSION] [-b BLACK_REFERENCE] [-w WHITE_REFERENCE] [-a] [--red INT] [--green INT] [--blue INT] [-o OUTPUT_DIR] [--width INT] [--height INT] [-n] [-v] Fake RGB image generator for HSI files. optional arguments: -h, --help show this help message and exit -i INPUT_DIR [INPUT_DIR ...], --input_dir INPUT_DIR [INPUT_DIR ...] Path to the scan file (ENVI format) (default: None) -r, --recursive whether to traverse the directories recursively (default: False) -e EXTENSION, --extension EXTENSION The file extension to look for (default: .hdr) -b BLACK_REFERENCE, --black_reference BLACK_REFERENCE Path to the black reference file (ENVI format) (default: None) -w WHITE_REFERENCE, --white_reference WHITE_REFERENCE Path to the white reference file (ENVI format) (default: None) -a, --autodetect_channels whether to determine the channels from the meta-data (overrides the manually specified channels) (default: False) --red INT the wave length to use for the red channel (0-based) (default: 0) --green INT the wave length to use for the green channel (0-based) (default: 0) --blue INT the wave length to use for the blue channel (0-based) (default: 0) -o OUTPUT_DIR, --output_dir OUTPUT_DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) --width INT the width to scale the images to (<= 0 uses image dimension) (default: 0) --height INT the height to scale the images to (<= 0 uses image dimension) (default: 0) -n, --dry_run whether to omit saving the PNG images (default: False) -v, --verbose whether to be more verbose with the output (default: False)","title":"Command-line"},{"location":"happy/happy_tools/happy-mat-info/","text":"Command-line # usage: happy-mat-info [-h] matfile Load and display structs from a MATLAB file. positional arguments: matfile Path to the MATLAB file optional arguments: -h, --help show this help message and exit","title":"happy-mat-info"},{"location":"happy/happy_tools/happy-mat-info/#command-line","text":"usage: happy-mat-info [-h] matfile Load and display structs from a MATLAB file. positional arguments: matfile Path to the MATLAB file optional arguments: -h, --help show this help message and exit","title":"Command-line"},{"location":"happy/happy_tools/happy-opex2happy/","text":"Command-line # usage: happy-opex2happy [-h] -i DIR [DIR ...] [-o DIR] -f {flat,dir-tree} -l LABELS [-I] [-n] [-v] Turns annotations (PNG and OPEX JSON) into Happy ENVI format. optional arguments: -h, --help show this help message and exit -i DIR [DIR ...], --input_dir DIR [DIR ...] Path to the PNG/OPEX files (default: None) -o DIR, --output_dir DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) -f {flat,dir-tree}, --output_format {flat,dir-tree} Defines how to store the data in the output directory. (default: flat) -l LABELS, --labels LABELS The comma-separated list of object labels to export ('Background' is automatically added). (default: None) -I, --include_input whether to copy the PNG/JSON file across to the output dir (default: False) -n, --dry_run whether to omit saving the PNG images (default: False) -v, --verbose whether to be more verbose with the output (default: False)","title":"happy-opex2happy"},{"location":"happy/happy_tools/happy-opex2happy/#command-line","text":"usage: happy-opex2happy [-h] -i DIR [DIR ...] [-o DIR] -f {flat,dir-tree} -l LABELS [-I] [-n] [-v] Turns annotations (PNG and OPEX JSON) into Happy ENVI format. optional arguments: -h, --help show this help message and exit -i DIR [DIR ...], --input_dir DIR [DIR ...] Path to the PNG/OPEX files (default: None) -o DIR, --output_dir DIR The directory to store the fake RGB PNG images instead of alongside the HSI images. (default: None) -f {flat,dir-tree}, --output_format {flat,dir-tree} Defines how to store the data in the output directory. (default: flat) -l LABELS, --labels LABELS The comma-separated list of object labels to export ('Background' is automatically added). (default: None) -I, --include_input whether to copy the PNG/JSON file across to the output dir (default: False) -n, --dry_run whether to omit saving the PNG images (default: False) -v, --verbose whether to be more verbose with the output (default: False)","title":"Command-line"},{"location":"happy/happy_tools/happy-plot-preproc/","text":"Command-line # usage: happy-plot-preproc [-h] [--pixels PIXELS] foldername Plot set of pixels with various pre-processing. positional arguments: foldername Folder containing HappyData files optional arguments: -h, --help show this help message and exit --pixels PIXELS Number of random pixels to select (default: 100)","title":"happy-plot-preproc"},{"location":"happy/happy_tools/happy-plot-preproc/#command-line","text":"usage: happy-plot-preproc [-h] [--pixels PIXELS] foldername Plot set of pixels with various pre-processing. positional arguments: foldername Folder containing HappyData files optional arguments: -h, --help show this help message and exit --pixels PIXELS Number of random pixels to select (default: 100)","title":"Command-line"},{"location":"happy/happy_tools/happy-scikit-regression-build/","text":"Command-line # usage: happy-scikit-regression-build [-h] [--repeat_num REPEAT_NUM] happy_data_base_dir regression_method regression_params target_value happy_splitter_file output_folder Evaluate regression model on Happy Data using specified splits and pixel selector. positional arguments: happy_data_base_dir Directory containing the Happy Data files regression_method Regression method name regression_params JSON string containing regression parameters target_value Target value column name happy_splitter_file Happy Splitter file output_folder Output JSON file to store the predictions optional arguments: -h, --help show this help message and exit --repeat_num REPEAT_NUM Repeat number (default: 1) (default: 0)","title":"happy-scikit-regression-build"},{"location":"happy/happy_tools/happy-scikit-regression-build/#command-line","text":"usage: happy-scikit-regression-build [-h] [--repeat_num REPEAT_NUM] happy_data_base_dir regression_method regression_params target_value happy_splitter_file output_folder Evaluate regression model on Happy Data using specified splits and pixel selector. positional arguments: happy_data_base_dir Directory containing the Happy Data files regression_method Regression method name regression_params JSON string containing regression parameters target_value Target value column name happy_splitter_file Happy Splitter file output_folder Output JSON file to store the predictions optional arguments: -h, --help show this help message and exit --repeat_num REPEAT_NUM Repeat number (default: 1) (default: 0)","title":"Command-line"},{"location":"happy/happy_tools/happy-scikit-unsupervised-build/","text":"Command-line # usage: happy-scikit-unsupervised-build [-h] [--repeat_num REPEAT_NUM] data_folder clusterer_name clusterer_params target_value happy_splitter_file output_folder Evaluate clustering on hyperspectral data using specified clusterer and pixel selector. positional arguments: data_folder Directory containing the hyperspectral data clusterer_name Clusterer name (e.g., kmeans, agglomerative, spectral, dbscan, meanshift) clusterer_params JSON string containing clusterer parameters target_value Target value column name happy_splitter_file Happy Splitter file output_folder Output JSON file to store the predictions optional arguments: -h, --help show this help message and exit --repeat_num REPEAT_NUM Repeat number (default: 1) (default: 0)","title":"happy-scikit-unsupervised-build"},{"location":"happy/happy_tools/happy-scikit-unsupervised-build/#command-line","text":"usage: happy-scikit-unsupervised-build [-h] [--repeat_num REPEAT_NUM] data_folder clusterer_name clusterer_params target_value happy_splitter_file output_folder Evaluate clustering on hyperspectral data using specified clusterer and pixel selector. positional arguments: data_folder Directory containing the hyperspectral data clusterer_name Clusterer name (e.g., kmeans, agglomerative, spectral, dbscan, meanshift) clusterer_params JSON string containing clusterer parameters target_value Target value column name happy_splitter_file Happy Splitter file output_folder Output JSON file to store the predictions optional arguments: -h, --help show this help message and exit --repeat_num REPEAT_NUM Repeat number (default: 1) (default: 0)","title":"Command-line"},{"location":"happy/happy_tools/happy-splitter/","text":"Command-line # usage: happy-splitter [-h] [--num_repeats NUM_REPEATS] [--num_folds NUM_FOLDS] [--train_percent TRAIN_PERCENT] [--validation_percent VALIDATION_PERCENT] [--use_regions] [--holdout_percent HOLDOUT_PERCENT] [--output_file OUTPUT_FILE] happy_base_folder Generate train/validation/test splits for Happy data. positional arguments: happy_base_folder Path to the Happy base folder optional arguments: -h, --help show this help message and exit --num_repeats NUM_REPEATS Number of repeats (default: 1) --num_folds NUM_FOLDS Number of folds (default: 1) --train_percent TRAIN_PERCENT Percentage of data in the training set (default: 70.0) --validation_percent VALIDATION_PERCENT Percentage of data in the validation set (default: 10.0) --use_regions Use regions in generating splits (default: False) --holdout_percent HOLDOUT_PERCENT Percentage of data to hold out as a holdout set (default: None) --output_file OUTPUT_FILE Path to the output split file (default: output_split.json)","title":"Command-line"},{"location":"happy/happy_tools/happy-splitter/#command-line","text":"usage: happy-splitter [-h] [--num_repeats NUM_REPEATS] [--num_folds NUM_FOLDS] [--train_percent TRAIN_PERCENT] [--validation_percent VALIDATION_PERCENT] [--use_regions] [--holdout_percent HOLDOUT_PERCENT] [--output_file OUTPUT_FILE] happy_base_folder Generate train/validation/test splits for Happy data. positional arguments: happy_base_folder Path to the Happy base folder optional arguments: -h, --help show this help message and exit --num_repeats NUM_REPEATS Number of repeats (default: 1) --num_folds NUM_FOLDS Number of folds (default: 1) --train_percent TRAIN_PERCENT Percentage of data in the training set (default: 70.0) --validation_percent VALIDATION_PERCENT Percentage of data in the validation set (default: 10.0) --use_regions Use regions in generating splits (default: False) --holdout_percent HOLDOUT_PERCENT Percentage of data to hold out as a holdout set (default: None) --output_file OUTPUT_FILE Path to the output split file (default: output_split.json)","title":"Command-line"},{"location":"happy/happy_tools/installation/","text":"Prerequisites # Windows/WSL : Ubuntu 2022.04.x from the Microsoft store Python Virtual environments sudo apt install virtualenv python3-tk Installation # In the home directory, create a Python virtual environment in directory happy with access to the system-wide installed libraries: virtualenv --system-site-packages -p /usr/bin/python3 happy Install the happy-tools straight from the repository: ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git Updating # Once installed, you can update the library as follows: ./happy/bin/pip uninstall -y happy-tools ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git Uninstall # You can completely remove the tools by simply deleting the happy directory: rm -Rf ./happy","title":"Installation"},{"location":"happy/happy_tools/installation/#prerequisites","text":"Windows/WSL : Ubuntu 2022.04.x from the Microsoft store Python Virtual environments sudo apt install virtualenv python3-tk","title":"Prerequisites"},{"location":"happy/happy_tools/installation/#installation","text":"In the home directory, create a Python virtual environment in directory happy with access to the system-wide installed libraries: virtualenv --system-site-packages -p /usr/bin/python3 happy Install the happy-tools straight from the repository: ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git","title":"Installation"},{"location":"happy/happy_tools/installation/#updating","text":"Once installed, you can update the library as follows: ./happy/bin/pip uninstall -y happy-tools ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git","title":"Updating"},{"location":"happy/happy_tools/installation/#uninstall","text":"You can completely remove the tools by simply deleting the happy directory: rm -Rf ./happy","title":"Uninstall"},{"location":"happy/happy_tools_keras/","text":"Additional tools utilizing the Keras deep learning library, available through happy-tools-keras : happy-keras-pixel-regression-build - evaluate a Keras-based pixel regression model happy-keras-segmentation-build - builds a Keras-based pixel segmentation model happy-keras-unsupervised-build - builds a Keras-based pixel segmentation model These tools are available from the Python virtual environment that they were installed. E.g., when following the installation instructions on this website, the tools would be located in the following directory in the user's home folder: happy/bin","title":"Overview"},{"location":"happy/happy_tools_keras/happy-keras-pixel-regression-build/","text":"Command-line # usage: happy-keras-pixel-regression-build [-h] data_folder target happy_splitter_file output_folder Evaluate a Keras-based pixel regression model. positional arguments: data_folder Path to the data folder target Name of the target variable happy_splitter_file Path to JSON file containing splits output_folder Path to the output folder optional arguments: -h, --help show this help message and exit","title":"happy-keras-pixel-regression-build"},{"location":"happy/happy_tools_keras/happy-keras-pixel-regression-build/#command-line","text":"usage: happy-keras-pixel-regression-build [-h] data_folder target happy_splitter_file output_folder Evaluate a Keras-based pixel regression model. positional arguments: data_folder Path to the data folder target Name of the target variable happy_splitter_file Path to JSON file containing splits output_folder Path to the output folder optional arguments: -h, --help show this help message and exit","title":"Command-line"},{"location":"happy/happy_tools_keras/happy-keras-segmentation-build/","text":"Command-line # usage: keras_segmentation_build.py [-h] data_folder target happy_splitter_file output_folder Build a Keras-based pixel segmentation model. positional arguments: data_folder Path to the data folder target Name of the target variable happy_splitter_file Path to JSON file containing splits output_folder Path to the output folder optional arguments: -h, --help show this help message and exit","title":"happy-keras-segmentation-build"},{"location":"happy/happy_tools_keras/happy-keras-segmentation-build/#command-line","text":"usage: keras_segmentation_build.py [-h] data_folder target happy_splitter_file output_folder Build a Keras-based pixel segmentation model. positional arguments: data_folder Path to the data folder target Name of the target variable happy_splitter_file Path to JSON file containing splits output_folder Path to the output folder optional arguments: -h, --help show this help message and exit","title":"Command-line"},{"location":"happy/happy_tools_keras/happy-keras-unsupervised-build/","text":"Command-line # usage: happy-keras-unsupervised-build [-h] data_folder target happy_splitter_file output_folder Build a Keras-based pixel segmentation model. positional arguments: data_folder Path to the data folder target Name of the target variable happy_splitter_file Path to JSON file containing splits output_folder Path to the output folder optional arguments: -h, --help show this help message and exit","title":"happy-keras-unsupervised-build"},{"location":"happy/happy_tools_keras/happy-keras-unsupervised-build/#command-line","text":"usage: happy-keras-unsupervised-build [-h] data_folder target happy_splitter_file output_folder Build a Keras-based pixel segmentation model. positional arguments: data_folder Path to the data folder target Name of the target variable happy_splitter_file Path to JSON file containing splits output_folder Path to the output folder optional arguments: -h, --help show this help message and exit","title":"Command-line"},{"location":"happy/happy_tools_keras/installation/","text":"Prerequisites # Windows/WSL : Ubuntu 2022.04.x from the Microsoft store Python Virtual environments sudo apt install virtualenv python3-tk Installation # In the home directory, create a Python virtual environment in directory happy with access to the system-wide installed libraries: virtualenv --system-site-packages -p /usr/bin/python3 happy Install the happy-tools straight from the repository: ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git ./happy/bin/pip install git+https://github.com/wairas/happy-tools-keras.git Updating # Once installed, you can update the library as follows: ./happy/bin/pip uninstall -y happy-tools happy-tools-keras ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git ./happy/bin/pip install git+https://github.com/wairas/happy-tools-keras.git Uninstall # You can completely remove the tools by simply deleting the happy directory: rm -Rf ./happy","title":"Installation"},{"location":"happy/happy_tools_keras/installation/#prerequisites","text":"Windows/WSL : Ubuntu 2022.04.x from the Microsoft store Python Virtual environments sudo apt install virtualenv python3-tk","title":"Prerequisites"},{"location":"happy/happy_tools_keras/installation/#installation","text":"In the home directory, create a Python virtual environment in directory happy with access to the system-wide installed libraries: virtualenv --system-site-packages -p /usr/bin/python3 happy Install the happy-tools straight from the repository: ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git ./happy/bin/pip install git+https://github.com/wairas/happy-tools-keras.git","title":"Installation"},{"location":"happy/happy_tools_keras/installation/#updating","text":"Once installed, you can update the library as follows: ./happy/bin/pip uninstall -y happy-tools happy-tools-keras ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git ./happy/bin/pip install git+https://github.com/wairas/happy-tools-keras.git","title":"Updating"},{"location":"happy/happy_tools_keras/installation/#uninstall","text":"You can completely remove the tools by simply deleting the happy directory: rm -Rf ./happy","title":"Uninstall"}]}