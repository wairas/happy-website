{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Hyper-spectral Imaging Group at the University of Waikato.","title":"Home"},{"location":"people/","text":"Hyper-spectral imaging group: AProf Melanie Ooi Dale Fletcher Peter Reutemann","title":"People"},{"location":"publications/","text":"2023 # Abeysekera SK, Robinson A, Ooi MPL, Kuang YC, Manley-Harris M, Holmes W, Hirst E, Nowak J, Caddie M, Steinhorn G, Demidenko S. Sparse reproducible machine learning for near infrared hyperspectral imaging: Estimating the tetrahydrocannabinolic acid concentration in Cannabis sativa L. 1 Feb 2023. Industrial Crops and Products, 10.1016/j.indcrop.2022.116137","title":"Publications"},{"location":"publications/#2023","text":"Abeysekera SK, Robinson A, Ooi MPL, Kuang YC, Manley-Harris M, Holmes W, Hirst E, Nowak J, Caddie M, Steinhorn G, Demidenko S. Sparse reproducible machine learning for near infrared hyperspectral imaging: Estimating the tetrahydrocannabinolic acid concentration in Cannabis sativa L. 1 Feb 2023. Industrial Crops and Products, 10.1016/j.indcrop.2022.116137","title":"2023"},{"location":"happy/","text":"Python libraries and Docker images for hyper-spectral data processing and modelling. ADAMS happy-tools Segment Anything","title":"Introduction"},{"location":"happy/adams/","text":"ADAMS-based framework that can be used for annotating images using its powerful workflow engine. Requirements # OpenJDK 11+ Windows: adoptium.net/temurin Debian/Ubuntu: sudo apt install openjdk-11-jdk Installation # Download a snapshot (in ZIP format) adams.cms.waikato.ac.nz/snapshots/happy/ Unzip the ZIP archive and rename the generated directory to happy-adams Starting the application # Start the user interface with: Windows: happy-adams\\bin\\start_gui.bat Linux: happy-adams/bin/start_gui.sh Available flows # Of the flows that are come with the Happy ADAMS framework, the following ones are relevant to the Happy project: adams-imaging-annotate_objects.flow - generating annotations for object detection (bounding box or polygon) adams-imaging-image_segmentation_annotation.flow - generating annotations for image segmentation (pixel-level classification) adams-imaging-ext_run-sam.flow - downloads and runs SAM (Segment Anything Model) via Docker (can be used within the above two flows as a separate annotation tool) Tutorials # Instructions on how to use these flows are available from the Applied Deep Learning website, specifically: object detection image segmentation","title":"ADAMS"},{"location":"happy/adams/#requirements","text":"OpenJDK 11+ Windows: adoptium.net/temurin Debian/Ubuntu: sudo apt install openjdk-11-jdk","title":"Requirements"},{"location":"happy/adams/#installation","text":"Download a snapshot (in ZIP format) adams.cms.waikato.ac.nz/snapshots/happy/ Unzip the ZIP archive and rename the generated directory to happy-adams","title":"Installation"},{"location":"happy/adams/#starting-the-application","text":"Start the user interface with: Windows: happy-adams\\bin\\start_gui.bat Linux: happy-adams/bin/start_gui.sh","title":"Starting the application"},{"location":"happy/adams/#available-flows","text":"Of the flows that are come with the Happy ADAMS framework, the following ones are relevant to the Happy project: adams-imaging-annotate_objects.flow - generating annotations for object detection (bounding box or polygon) adams-imaging-image_segmentation_annotation.flow - generating annotations for image segmentation (pixel-level classification) adams-imaging-ext_run-sam.flow - downloads and runs SAM (Segment Anything Model) via Docker (can be used within the above two flows as a separate annotation tool)","title":"Available flows"},{"location":"happy/adams/#tutorials","text":"Instructions on how to use these flows are available from the Applied Deep Learning website, specifically: object detection image segmentation","title":"Tutorials"},{"location":"happy/sam/","text":"Facebook's Segment Anything are pretrained models that perform image segmentation on RGB images and can aid the human in the annotation process. Prerequisites # Linux # docker redis-server ( sudo apt install redis-server ) Windows # WSL2 using Ubuntu 20.04 or 22.04 docker ( instructions ) redis-server ( sudo apt install redis-server ) Directories # sam | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM models You can create the structure using the following command: mkdir -p sam/cache \\ mkdir -p sam/models Pretrained models # Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam/models directory, run the following command: wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth Service scripts (WSL2 without Docker Desktop UI) # Create a bash script happy_sam_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_sam_start.sh Create a bash script happy_sam_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happy_sam_stop.sh SAM scripts # In the sam directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam:2023-04-16_cuda11.6 \\ sam_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_vit_l_0b3195.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]am_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh . Starting # Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_sam_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background. SAM # In the sam directory, execute the start.sh script. Stopping # SAM # In the sam directory, execute the stop.sh script. Docker and Redis (WSL2 without Docker Desktop UI) # sudo /usr/local/bin/happy_sam_stop.sh NB: This will also stop any running SAM process.","title":"SAM"},{"location":"happy/sam/#prerequisites","text":"","title":"Prerequisites"},{"location":"happy/sam/#linux","text":"docker redis-server ( sudo apt install redis-server )","title":"Linux"},{"location":"happy/sam/#windows","text":"WSL2 using Ubuntu 20.04 or 22.04 docker ( instructions ) redis-server ( sudo apt install redis-server )","title":"Windows"},{"location":"happy/sam/#directories","text":"sam | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the SAM models You can create the structure using the following command: mkdir -p sam/cache \\ mkdir -p sam/models","title":"Directories"},{"location":"happy/sam/#pretrained-models","text":"Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. From within the sam/models directory, run the following command: wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth","title":"Pretrained models"},{"location":"happy/sam/#service-scripts-wsl2-without-docker-desktop-ui","text":"Create a bash script happy_sam_start.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with sudo chmod a+x happy_sam_start.sh Create a bash script happy_sam_stop.sh in /usr/local/bin with the following content: #!/bin/bash killall redis-server killall dockerd Make the script executable with sudo chmod a+x happy_sam_stop.sh","title":"Service scripts (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/#sam-scripts","text":"In the sam directory, create script start.sh with the following content: #!/bin/bash scriptdir=`dirname -- \"$0\";` docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v $scriptdir/cache:/.cache \\ -v $scriptdir:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam:2023-04-16_cuda11.6 \\ sam_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_vit_l_0b3195.pth \\ --model_type vit_l \\ --verbose And make executable with chmod a+x start.sh . Next, create a script called stop.sh with the following content: #!/bin/bash ids=`ps a | grep [s]am_predict_redis | sed s/\"^[ ]*\"//g | cut -f1 -d\" \"` for id in $ids do kill -9 $id done And make executable with chmod a+x stop.sh .","title":"SAM scripts"},{"location":"happy/sam/#starting","text":"","title":"Starting"},{"location":"happy/sam/#docker-and-redis-wsl2-without-docker-desktop-ui","text":"sudo /usr/local/bin/happy_sam_start.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/sam/#sam","text":"In the sam directory, execute the start.sh script.","title":"SAM"},{"location":"happy/sam/#stopping","text":"","title":"Stopping"},{"location":"happy/sam/#sam_1","text":"In the sam directory, execute the stop.sh script.","title":"SAM"},{"location":"happy/sam/#docker-and-redis-wsl2-without-docker-desktop-ui_1","text":"sudo /usr/local/bin/happy_sam_stop.sh NB: This will also stop any running SAM process.","title":"Docker and Redis (WSL2 without Docker Desktop UI)"},{"location":"happy/happy_tools/","text":"happy-tools contains several command-line utilities and graphical viewers for HSI files: envi-viewer - for viewing ENVI HSI images happy-hsi2csv - converts HSI images into CSV happy-hsi2rbg - generates fake RGB PNG files from HSI images","title":"Tools"},{"location":"happy/happy_tools/installation/","text":"Prerequisites # Python Virtual environments sudo apt install virtualenv python3-tk Installation # In the home directory, create a Python virtual environment in directory happy with access to the system-wide installed libraries: virtualenv --system-site-packages -p /usr/bin/python3 happy Install the happy-tools straight from the repository: ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git Updating # Once installed, you can update the library as follows: ./happy/bin/pip uninstall happy-tools ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git","title":"Installation"},{"location":"happy/happy_tools/installation/#prerequisites","text":"Python Virtual environments sudo apt install virtualenv python3-tk","title":"Prerequisites"},{"location":"happy/happy_tools/installation/#installation","text":"In the home directory, create a Python virtual environment in directory happy with access to the system-wide installed libraries: virtualenv --system-site-packages -p /usr/bin/python3 happy Install the happy-tools straight from the repository: ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git","title":"Installation"},{"location":"happy/happy_tools/installation/#updating","text":"Once installed, you can update the library as follows: ./happy/bin/pip uninstall happy-tools ./happy/bin/pip install git+https://github.com/wairas/happy-tools.git","title":"Updating"}]}