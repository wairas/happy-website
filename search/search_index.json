{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Hyper-spectral Imaging Group at the University of Waikato.","title":"Home"},{"location":"people/","text":"Hyper-spectral imaging group: AProf Melanie Ooi Dale Fletcher Peter Reutemann","title":"People"},{"location":"publications/","text":"2023 # Abeysekera SK, Robinson A, Ooi MPL, Kuang YC, Manley-Harris M, Holmes W, Hirst E, Nowak J, Caddie M, Steinhorn G, Demidenko S. Sparse reproducible machine learning for near infrared hyperspectral imaging: Estimating the tetrahydrocannabinolic acid concentration in Cannabis sativa L. 1 Feb 2023. Industrial Crops and Products, 10.1016/j.indcrop.2022.116137","title":"Publications"},{"location":"publications/#2023","text":"Abeysekera SK, Robinson A, Ooi MPL, Kuang YC, Manley-Harris M, Holmes W, Hirst E, Nowak J, Caddie M, Steinhorn G, Demidenko S. Sparse reproducible machine learning for near infrared hyperspectral imaging: Estimating the tetrahydrocannabinolic acid concentration in Cannabis sativa L. 1 Feb 2023. Industrial Crops and Products, 10.1016/j.indcrop.2022.116137","title":"2023"},{"location":"happy/","text":"Python libraries and Docker images for hyper-spectral data processing and modelling. ADAMS Docker images happy-tools Segment Anything","title":"Introduction"},{"location":"happy/adams/","text":"ADAMS-based workflow module.","title":"ADAMS"},{"location":"happy/docker_images/","text":"The following Docker images are available: happy-gdal-docker - GDAL docker image happy-spy-docker - Spectral Python library image","title":"Docker images"},{"location":"happy/happy_tools/","text":"happy-tools contains several command-line utilities and a graphical viewer for HSI ENVI files. Installation # pip install git+https://github.com/wairas/happy-tools.git","title":"happy-tools"},{"location":"happy/happy_tools/#installation","text":"pip install git+https://github.com/wairas/happy-tools.git","title":"Installation"},{"location":"happy/sam/","text":"Facebook's Segment Anything are pretrained models that perform image segmentation on RGB images and can aid the human in the annotation process. Prerequisites # Linux # docker redis-server Windows # WSL2 using Ubuntu 20.04 or 22.04 docker ( instructions ) redis-server Directories # sam | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the You can create the structure using the following command: mkdir -p sam/cache \\ mkdir -p sam/models Pretrained models # Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below. Launching Docker and Redis under WLS2 # Create a bash script happy_sam.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with chmod a+x happy_sam.sh Launching Docker and Redis under WLS2 # sudo /usr/bin/happy_sam.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background. Launching SAM # In a terminal, run the following command from within the sam directory to launch the SAM model (which communicates via the sam_in and sam_out Redis channels): docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`/cache:/.cache -v `pwd`:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam:2023-04-16_cuda11.6 \\ sam_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_vit_l_0b3195.pth \\ --model_type vit_l \\ --verbose","title":"SAM"},{"location":"happy/sam/#prerequisites","text":"","title":"Prerequisites"},{"location":"happy/sam/#linux","text":"docker redis-server","title":"Linux"},{"location":"happy/sam/#windows","text":"WSL2 using Ubuntu 20.04 or 22.04 docker ( instructions ) redis-server","title":"Windows"},{"location":"happy/sam/#directories","text":"sam | +-- cache # cache directory for Pytorch-related files | +-- models # for storing the You can create the structure using the following command: mkdir -p sam/cache \\ mkdir -p sam/models","title":"Directories"},{"location":"happy/sam/#pretrained-models","text":"Pretrained models can be downloaded from here , with the medium-sized vit_l being the recommended one (requires <6GB GPU RAM). vit_l is used in the commands below.","title":"Pretrained models"},{"location":"happy/sam/#launching-docker-and-redis-under-wls2","text":"Create a bash script happy_sam.sh in /usr/local/bin with the following content: #!/bin/bash redis-server & dockerd & seq 10 | xargs -I{} sh -c \"echo waiting...; sleep 1;\" Make the script executable with chmod a+x happy_sam.sh","title":"Launching Docker and Redis under WLS2"},{"location":"happy/sam/#launching-docker-and-redis-under-wls2_1","text":"sudo /usr/bin/happy_sam.sh Wait till the Waiting... output stops, which waits for about 10 seconds after the Docker daemon starts in the background.","title":"Launching Docker and Redis under WLS2"},{"location":"happy/sam/#launching-sam","text":"In a terminal, run the following command from within the sam directory to launch the SAM model (which communicates via the sam_in and sam_out Redis channels): docker run --pull always --rm \\ -u $(id -u):$(id -g) -e USER=$USER \\ -v `pwd`/cache:/.cache -v `pwd`:/workspace \\ --gpus=all --net=host \\ -t waikatodatamining/pytorch-sam:2023-04-16_cuda11.6 \\ sam_predict_redis \\ --redis_in sam_in \\ --redis_out sam_out \\ --model /workspace/models/sam_vit_l_0b3195.pth \\ --model_type vit_l \\ --verbose","title":"Launching SAM"}]}